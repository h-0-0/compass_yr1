---
title: "lab_1"
output: html_document
date: "2022-09-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 1: Linear Least Squares Estimation on Prostate Cancer Dataset
In this code we will investigate the "Prostate cancer dataset". We will produce a linear Least Squares (LS) solver to produce a model for the dataset that predicts the outcome given the predictor variables. We will then calculate the cross validation error of our model and then analyse the importance of each predictor variable in our model. From this analysis we find that the lcavol, lweight and svi predictor variables do not impact performance when removed from the model and therefore suggest that we can reduce the dimensionality of our problem by removing said predictor variables from our model.

Throughout this document we will make sure to keep the code reproducible and will be conducting literate programming in order to make the code easily readable and understandable.


## Finding the Least squares estimator
We begin by loading in the dataset directly from the URL so this can be run on any computer without first downloading the file. We also print out the first 6 rows to check the structure of the data.
```{r cars}
D <- read.table(url("https://hastie.su.domains/ElemStatLearn/datasets/prostate.data")); head(D)
```

We extract the predictor variable y from the dataset and print it to check it has the correct structure.
```{r}
y <- D$lpsa; y
```

We now create a function which given the dataset, D, will return the model matrix X.
```{r}
mm <- function(D) {
  X <- as.matrix(D[ 1:(ncol(D)-2) ])
  ones <- rep(1, nrow(D))
  X <- cbind(ones, X)
  X
}
```

And we now use that function to obtain the model matrix X from the dataset D.
```{r}
X <- mm(D); head(X)
```

Now we have the model matrix, X, and predictor variable, y, we have everything we need to find the LS estimate. 

We now write a function that takes in the model matrix and target variable and returns the LS solution.
```{r}
LLS <- function(X, y) {
  w <- solve(t(X) %*% X) %*% t(X)  %*% y
  w
}
```

An we now call that function using our X and y to give us the LS estimator. We also print our solution.
```{r}
w_LS <- LLS(X, y); w_LS
```

## Cross Validation
We now are going to assess the performance of our estimator by calculating the cross-validation error.

We begin by randomly shuffling our data and denoting this shuffled data by D_dash
```{r}
D_dash <- D[sample(nrow(D)),] ; head(D_dash)
```
We now create a list which indexes the rows in our dataset, we will use this to select groups of certain rows from the dataset, hence what we have effectively done here is partition the dataset (randomly - as we shuffled the dataset previously) into groups. We split the dataset into 10 groups as we will be performing 10-fold cross validation.
```{r}
subsets <- cut(seq(1,nrow(D)), breaks = 10, labels = FALSE) ; subsets
```
We now write a function that will carry out 10-fold cross validation given the dataset, D, and the predictor variable, y.
```{r}
cross_val <- function(D, y){
 
  # We randomly shuffle the indices of the rows and using these randomized indices shuffle the rows of the dataset and the target variable (in the same order)
  ind <- sample(nrow(D))
  D_dash <- D[paste(ind),]
  y_dash <- y[ind]
  
  # We now create a list which indexes the rows in our dataset, we will use this to select groups of certain rows from the dataset, hence what we have effectively done here is partition the dataset (randomly - as we shuffled the dataset previously) into groups. We split the dataset into 10 groups as we will be performing 10-fold cross validation.
  subsets <- cut(seq(1,nrow(D)), breaks = 10, labels = FALSE)
  
  
  # We create a vector to store our cross-val errors
  errors <- c()
  
  # Loop for carrying out 10-fold cross-val
  for(i in 1:10){
    
    # We segment our data into testing, D.test, and training, D.train, datasets
    testIndexes <- which(subsets==i,arr.ind=TRUE)
    D.test <- D_dash[testIndexes, ]
    D.train <- D_dash[-testIndexes, ]
    
    # We get the model matrix and predictor variables for the training data
    # and then we find the LS estimator
    X.train <- mm(D.train) 
    y.train <- y_dash[-testIndexes]
    w <- LLS(X.train, y.train) 
    
    # We get the model matrix and predictor variables for the testing data
    # and then we calculate the least squares testing error
    X.test <- mm(D.test)
    y.test <- y_dash[testIndexes]
    test.error <- norm(y.test - X.test %*% w, type="2")**2
    
    # We add the testing error to our error vector
    errors[i] <- test.error
    
  }
  
  # We find and return the cross-val error
  error.CV <- sum(errors)/10
  error.CV
}
```

Finally, we carry out cross validation using our data and find the cross-validation error.
```{r}
cross_val(D,y)
```


## Dimensionality reduction
Now we would like to see if all the predictor variables are important, to check this for each predictor variable we will remove it from the dataset and calculate the cross-validation error. Examining the cross-validation errors we can then check to see if removing certain features corresponds to obtaining larger errors and therefore if certain features are more crucial to keep in the model than others. 

Here we loop through the dataset 8 times, each time removing a different column (predictor variable) and then calculating and storing the error in a vector. We do this 100 times and return the average cross-validation error.
```{r}
total_error <- rep(0, 8)
for(j in 1:100){
  errors_j <- c()
  for(i in 1:8){
    D_drop <- D[-i]
    errors_j[i] <-  cross_val(D_drop,y)
  }
  total_error <- total_error + errors_j
}
total_error/100
```
From our results we see that removing column 1 leads to a large increase in the cross-validation error. However, removing any of the other columns leads to more minor changes in the error.

This may suggest that the other features may not be needed in the model. To test this we will calculate the cross validation error obtained by the LS estimator by just using column 1 (lcavol).
```{r}
D_drop <- D[-c(2,3,4,5,6,7,8)]
cross_val(D_drop,y)
```
We notice that the cross-validation error produced is higher than when we had all the features present in our model. Looking back at the errors found when dropping features we notice that column 2 (lweight) and column 5 (svi) have larger errors. So lets do the same as before but now also dropping these features. 
```{r}
D_drop <- D[-c(3,4,6,7,8)]
cross_val(D_drop,y)
```
We see that the error we obtain is around the value we got when using all the features. This suggests that the lcavol, lweight and svi predictor variables are potentially not needed in our model for us to have good predictions for the outcome. Hence we can reduce the dimensionality of our problem (and need less observations/ produce a better model on the number of observations) whilst being able to produce just as good a model (than with all the features still present).
