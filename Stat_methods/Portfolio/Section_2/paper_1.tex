\section{Bias-Variance Decomposition}
What is the mathematical explanation of overfitting? Why is cross-validation a good measurement of the generalization of a prediction? we introduce a frequentist analysis to answer these questions. 

First we will assume an outcome, $\yi$, is generated by $\yi = g(\xui) +\ei$. Where $\gx : \mathbb{R}^{d} \rightarrow \mathbb{R}$ is a deterministic function, $\forall i, \ei$ independent of $\xui$ and $\E(\ei)=0$. For now for the sake of simplicity we also assume $\xui$ are fixed. We also partition the dataset, $D$, into training, $D_{0}$, and testing, $D_{1}$ datasets and will use least squares as our error/cost function, denoted $E$.

Whats better than the testing error on a specific dataset? One answer is the expectation of the testing error over the whole dataset. ie. $\E_{D}(E(D_{1}, \wls)) = \E_{D}( \sum_{i} [\yi - \prls]^{2} ) = \sum_{i} \E_{D}([\yi - \prls]^{2} | \xui)$. We can decompose this as follows (proof in \cref{proof:bv-decomp}):
\begin{definition} \label{definition:bv-decomp}
    \textbf{Bias-Variance Decomposition:}
    \begin{equation}
        \E_{D}([\yi - \prls]^{2} | \xui) = \V(\ei) + [g(\xui) - \E(\prls) | \xui]^{2} + \V(\prls | \xui)
    \end{equation}
\end{definition}
The $1^{st}$ term measures the randomness of our data generating process (this is beyond our control), the $2^{nd}$ term shows the accuracy of our expected prediction and the $3^{rd}$ term shows how easily our fitted prediction function is affected by the randomness of the dataset. When we are using a polynomial feature transform and increase b, the prediction function becomes more complex and hence the bias decreases, but conversely the variance increases. Note that we use the $\wls$ estimator here but the bias-variance decomposition holds true for any estimator, furthermore when we do use $\wls$ the bias equals zero. Also note that we can derive bias-variance Decompositions for different loss functions. A balance between trying to minimize the bias and variance gives us the \textbf{minimum expected error}.  

\subsection{Justifying K-fold cross-validation}
First we define the:
\begin{definition} \label{def:in-sample-error}
    \textbf{In-Sample Error:} \\
    This is the collective error of the training set, we can calculate it by averaging the expected error obtained over all the $\xui$'s in the training set,
    \begin{equation}
        \frac{1}{n} \sum_{i=1}^{n} \E( (\yi - \prls)^{2} | \xui) 
    \end{equation}
\end{definition}
In practice the in-sample error is not useful as we cannot calculate it, because we do not know $g(\bm{x})$ or the distribution of the $\ei$, instead we use the:
\begin{definition} \label{def:out-sample-error}
    \textbf{Out-sample Error:} \\
    The error over the whole distribution of $\bm{x}$,
    \begin{equation}
        \E_{\bm{x}} \E [ (\bm{y} - f(\bm{x} ; \wls) )^{2} | \bm{x}]
    \end{equation}
\end{definition}
Now that $\bm{x}$ is being treated as a random quantity, we have,
\begin{align}
    \E_{\bm{x}} \E [ (\bm{y} - f(\bm{x} ; \wls) )^{2} | \bm{x}] &{} 
    = \E_{\x} \E_{D_{1}} \E_{D_{0}} [ (y - \prlsv)^{2} | \x] \\
    &= \E_{p(\x)} \E_{p(y | \x)} \E_{D_{0}} [ (y - \prlsv)^{2}] \\
    &= \E_{D_{0}} \E_{p(y, \x)} [(y- \prlsv)^{2}]
\end{align}
Now, can we approximate the out-sample error? Suppose we have datasets $D^{(1)}, ... , D^{(K)}$ containing pairs $(y, \x)$ from $p(y,\x)$. Let $D^{(k)} := D_{0}^{(k)} \cup D_{1}^{(k)}$, the following holds under mild conditions (conditions that aren't too hard to obtain):
\begin{equation}
    \E_{D_{0}} \E_{p(y, \x)} [(y- \prlsv)^{2}] \approx \frac{1}{K} \sum_{k=1,...,K} \frac{1}{n_{k}} \sum_{(y,\x) \in D_{1}^{(k)}} (y - \prlsv^{(k)})^{2}
\end{equation}
Where $\prlsv^{(k)}$ denotes the prediction function trained on $D_{0}^{(k)}$ and where $n_{k}$ is the size of $D_{1}^{(k)}$. If we suppose that $D_{0}^{(k)}$ is the k-th split of an iid. dataset and $D_{1}^{(k)}$ is the rest of the dataset, then the above result provides some justification for the use of K-fold cross-validation. 