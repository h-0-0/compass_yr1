\section{Linear Classifiers}
In the portfolio 1 we saw how to conduct binary classification, now we will look at how to conduct \textbf{multi-class classification}. This is where we have an input $\x \in \mathbb{R}^{d}$ and an output $y \in \{1,...,K\}$.

The geometry of the problem in this case is more complicated than we had with binary classification, we can no longer simply check the sign of a single $f(\x)$ to classify. We could try introducing multiple functions. Let's say we have 3 classes, we could try introducing another function so now we have $f_{1}$ and $f_{2}$, we could then perform classification by checking the signs of both $f_{1}$ and $f_{2}$ and have this dictate the class. However, this can get confusing as for example in this case we have 4 possible outcomes, $\{+,-\}^{2}$, for the signs of our f's, however, we only have 3 classes. What about if we introduce pairwise binary classifiers, ie. we have $f_{i,j}$ classifies a point as either i or j and we have such a function for any pair of classes, then we classify a point as the majority vote given by the binary classifiers. The problem here is that all the classifiers may disagree in which case there is no majority vote.

Rather than relying on the sign of a function f to make predictions lets instead fit a vector valued function $\bm{f}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{K}$, where K is the number of classes. Given an input $\x$ our prediction is $\hat{k} = \argmax_{k} f^{(k)} (\x)$. Note that this no longer has a simple geometric interpretation anymore. 

\subsection{Least Squares Classifier}
We will first define the least squares classifier in the binary classification case and after extend it to the multi-class case:
\begin{definition}
    \textbf{Least Squares Binary Classifier:} \\
    We first perform LS on the data, ie. find:
    \begin{equation}
        \wls := \argmin_{\w} \sum_{i \in D} [y_{i} - f(\xui ; \w)]^{2}
    \end{equation}
    Then we can find the predicted label $\hat{y}:= \text{sign}(f(\xui;\wls))$
\end{definition}
Note that we can also use a feature transform for f as well, which would allow us to fit more complex classifiers, as data not separable in the original space may be separable in the feature space. In the multi-class case:
\begin{definition} \label{def:multi-class-LS-class}
    \textbf{Multi-class LS classification:} \\
    We use a \textbf{one-hot encoding} which is where we replace $y_{i}=k$ in our data with $\t_{i} \in \{0,1\}^{K}$ where all entries are 0 bar $t_{i}^{(k)}=1$. Then we have:
    \begin{equation}
        \Wls := \argmin_{\W} \sum_{i \in D} ||\t_{i} - f(\xui ; \W)||^{2}
    \end{equation}
    where $\W \in \mathbb{R}^{(d+1) \times K}$, $f(\x;\W)=\W^{T} \tilde{\x}$ and $\tilde{\x} := [\x^{T}, 1]^{T}$.\\
    Then our prediction $\hat{y} = \argmax_{k} f(\x;\W)^{(k)} = \argmax_{k}(\wls^{(k)})^{T} \tilde{\x}$, where $\w^{(k)}$ is the k-th column of $\W$.
\end{definition}
Although this method can work the square loss tends not to make sense in a classification task, as a point far away from the boundary (fit without that point) can dramatically affect our decision boundary, even if it is correctly classified by the decision boundary (fit without that point). Also, unlike with LS regression, LS classification lacks a probabilistic interpretation.

\subsection{Fisher Discriminant Analysis (FDA)}
Note that taking the inner product $\langle \w, \x \rangle$ embeds $\x$ onto a one-dimensional line along the $\w$ direction. We can say $\w$ gives a good embedding if the $\x$ it embeds are close together in the embedding if they are from the same class but far apart if they are from different classes. We can define these two properties we want from our embedding as:
\begin{definition}
    \textbf{Within-class Scatterness:} \\
    For embedding $\w^{T} \x$, the \textbf{embedding centre} for class k is:
    \begin{equation}
        \muhatk = \frac{1}{n_{k}} \sum_{i, y_{i}=k} \w^{T} \xui
    \end{equation}
    then the within-class scatterness of class k is:
    \begin{equation}
        s_{\w,k} := \sum_{i,y_{i}=k} (\w^{T} \xui - \muhatk)^{2}
    \end{equation}
\end{definition}
\begin{definition}
    \textbf{Between-class Scatterness:} \\
    The \textbf{embedded dataset centre} is:
    \begin{equation}
        \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} \w^{T} \xui
    \end{equation}
    then teh between-class scatterness is:
    \begin{equation}
        s_{b,k} = n_{k} (\muhatk - \hat{\mu})^{2}
    \end{equation}
    note the $n_{k}$ is needed to make $s_{b,k}$ the same scale as $s_{w,k}$.
\end{definition}
Then ideally we would like to maximize the between-class scatterness and minimize the within-class scatterness for all the classes, which is what the following does:
\begin{definition}
    \textbf{Fisher Discriminant Analysis (FDA):} \\
    \begin{equation}
        \max_{\w} [\sum_{k} s_{b,k} / \sum_{k} s_{\w,k} ]
    \end{equation}
\end{definition}
if $K=2$ then this has a simple solution: $\w := \bm{S}_{\w}^{-1} (\bm{\mu}_{+} - \bm{\mu}_{-})$, where $\bm{S}_{\w} := \sum_{k=1}^{K} \bm{S}_{k}$ and $\bm{S}_{k}$ is the sample covariance matrix of class k times $n_{k}$. However, note that the FDA does not learn a decision function f, the $\w_{FDA}$ obtained cannot be used directly by the prediction function to make a prediction. This is because (eg. in the binary case) $f(\x;\w_{FDA})>0$ does not mean that $\x$ is predicted as the positive class. The FDA also doesn't care about classification accuracy, ie. minimizing the FP or FN rate.

\subsection{Probabilistic (Generative) Classifiers}
How do we put a classification problem under a probabilistic framework? Let's say we would like to \textbf{minimize the expected loss:} $\hat{y} := \argmin_{y_{0}} \E_{p(y|\x)} [L(y,y_{0})|\x]$. To do this we need $p(y|\x)$. The \textbf{discriminative approach} would be to infer $p(y|\x)$ directly. Here, we will look at the \textbf{generative approach} which is where we note that $p(y|\x) \propto p(\x|y) p(y)$ and we infer $p(\x|y)$ \footnote{$p(y)$ is just the probability of the class being class y}.

\subsubsection{Continuous input} \label{subsubsection:Continuous input}
If $\x$ is a continuous variable then the Multi-Variate Normal (MVN) is a natural choice for the model of $p(\x|y)$, ie. we have $p(\x | y=k;\w) := N_{\bm{x}}(\bm{\mu_{k}}, \bm{\Sigma_{k}})$ (assuming iid data and that all classes have same covariance matrix). We can write the likelihood of the parameters over the data, D, as: $p(D|\w) = \prod_{i \in D}p(\xui , y_{i}|\w) = \prod_{i \in D} p(\xui | y_{i} ;\w)p(y_{i}) = \prod_{i \in D} N_{\xui} (\bm{\mu}_{y_{i}};\bm{\Sigma}) p(y_{i})$. Therefore our MLE's are: $\hat{\bm{\mu}_{1}, ..., \bm{\mu}_{K},\hat{\bm{\Sigma}}} := \argmax_{\bm{\mu}_{1}, ..., \bm{\mu}_{K},\hat{\bm{\Sigma}}} \sum_{i \in D} \log [ N_{\xui} (\bm{\mu}_{y_{i}} ; \bm{\Sigma}) p(y_{i})]$. If we plug in our estimates for $p(y_{i}=k)$ which are $\frac{n_{k}}{n}$, then we can find the MLE for $\hat{\bm{\mu}}_{k} := \frac{1}{n_{k}} \sum_{i \in D, y_{i}=k} \xui$. We can then plug in our $\hat{\bm{\mu}}_{k}$'s to work out: $\hat{\bm{\Sigma}} := \sum_{k=1,...,K} \frac{n_{k}}{n} \color{red} \frac{1}{n_{k}} \sum_{i \in D, y_{i}=k} (\xui - \bmuhatk)(\xui - \bmuhatk)^{T} \color{black}$, where we notice that in red is the MLE of the covariance of individual classes. Our prediction is $\hat{y} := \argmax_{y} p(y| \x ; \hat{\w}) \propto p(\x | y ; \hat{\w}) p(y)$. We can also prove that when using the shared covariance matrix MVN model, the decision boundary is piecewise-linear (\Cref{Proofs:linear-decision-boundary}).

What if we assume that for each class k there are different covariance matrices? then the MLE reduces to estimating individual $\bm{\mu}_{k}$ and $\bm{\Sigma}_{k}$. Note that in this case the decision boundary is no longer linear.

\subsubsection{Discrete input}
Here we will look at the case where $\x$ is discrete. Let $\x := [x^{(1)},...,x^{(d)}]^{T}$ and assume $x^{(1)},...,x^{(d)}$ follow a multinomial distribution, where each $x^{(i)}$ is a quantity for a feature. Using Bayes rule we have that $p(y=k| \x) = (p(\x|y=k) \cdot p(y=k)) / p(\x)$. We now introduce our "naive assumptions" which are that all the features in $\x$ are mutually independent, conditional on $y=k$. Using these assumptions we can write $p(y=k | \x) \propto p(y=k) \prod_{i=1}^{n} p(x_{i} |y=k)$ \footnote{We get this by discarding the $p(\x)$, which makes it proportional and then using our assumption on $p(\x|y=k)$}. Therefore $p(\x = \x_{0} |y=k) \propto \prod_{i=1,...,d} \beta(i|y=k)^{x_{0}^{(i)}}$, for some $\x_{0}$ where $\beta(i|y=k)$ is the probability feature i occurs in class k. It is easy to estimate this quantity:
\begin{equation}
    \beta(i|y=k) \approx \frac{\sum_{j \in D, y_{j}=k} x_{j}^{(i)}}{\sum_{j \in D, y_{j}=k} \sum_{l=1}^{d} x_{j}^{(l)}}
\end{equation}
Then our prediction is $\hat{y} := \argmax_{y} p(\x=\x_{0} |y) \cdot p(y)$ where as before $p(y=k)$ can be estimated as $\frac{n_{k}}{n}$ and note that $\beta(i|y=k)$ can be estimated by counting. This is called \textbf{Naive Bayes classification"}.