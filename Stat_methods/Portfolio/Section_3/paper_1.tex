\section{Linear Classifiers}
In the portfolio 1 we saw how to conduct binary classification, now we will look at how to conduct \textbf{multi-class classification}. This is where we have an input $\x \in \mathbb{R}^{d}$ and an output $y \in \{1,...,K\}$.

The geometry of the problem in this case is more complicated than we had with binary classification, we can no longer simply check the sign of a single $f(\x)$ to classify. We could try introducing multiple functions. Let's say we have 3 classes, we could try introducing another function so now we have $f_{1}$ and $f_{2}$, we could then perform classification by checking the signs of both $f_{1}$ and $f_{2}$ and have this dictate the class. However, this can get confusing as for example in this case we have 4 possible outcomes, $\{+,-\}^{2}$, for the signs of our f's, however, we only have 3 classes. What about if we introduce pairwise binary classifiers, ie. we have $f_{i,j}$ classifies a point as either i or j and we have such a function for any pair of classes, then we classify a point as the majority vote given by the binary classifiers. The problem here is that all the classifiers may disagree in which case there is no majority vote.

Rather than relying on the sign of a function f to make predictions lets instead fit a vector valued function $\bm{f}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{K}$, where K is the number of classes. Given an input $\x$ our prediction is $\hat{k} = \argmax_{k} f^{(k)} (\x)$. Note that this no longer has a simple geometric interpretation anymore. 

\subsection{Least Squares Classifier}
We will first define the least squares classifier in the binary classification case and after extend it to the multi-class case:
\begin{definition}
    \textbf{Least Squares Binary Classifier:} \\
    We first perform LS on the data, ie. find:
    \begin{equation}
        \wls := \argmin_{\w} \sum_{i \in D} [y_{i} - f(\xui ; \w)]^{2}
    \end{equation}
    Then we can find the predicted label $\hat{y}:= \text{sign}(f(\xui;\wls))$
\end{definition}
Note that we can also use a feature transform for f as well, which would allow us to fit more complex classifiers, as data not separable in the original space may be separable in the feature space. In the multi-class case:
\begin{definition}
    \textbf{Multi-class LS classification:} \\
    We use a \textbf{one-hot encoding} which is where we replace $y_{i}=k$ in our data with $\t_{i} \in \{0,1\}^{K}$ where all entries are 0 bar $t_{i}^{(k)}=1$. Then we have:
    \begin{equation}
        \Wls := \argmin_{\W} \sum_{i \in D} ||\t_{i} - f(\xui ; \W)||^{2}
    \end{equation}
    where $\W \in \mathbb{R}^{(d+1) \times K}$, $f(\x;\W)=\W^{T} \tilde{\x}$ and $\tilde{\x} := [\x^{T}, 1]^{T}$.\\
    Then our prediction $\hat{y} = \argmax_{k} f(\x;\W)^{(k)} = \argmax_{k}(\wls^{(k)})^{T} \tilde{\x}$, where $\w^{(k)}$ is the k-th column of $\W$.
\end{definition}
Although this method can work the square loss tends not to make sense in a classification task, as a point far away from the boundary (fit without that point) can dramatically affect our decision boundary, even if it is correctly classified by the decision boundary (fit without that point). Also, unlike with LS regression, LS classification lacks a probabilistic interpretation.

\subsection{Fisher Discriminant Analysis (FDA)}
Note that taking the inner product $\langle \w, \x \rangle$ embeds $\x$ onto a one-dimensional line along the $\w$ direction. We can say $\w$ gives a good embedding if the $\x$ it embeds are close together in the embedding if they are from the same class but far apart if they are from different classes. We can define these two properties we want from our embedding as:
\begin{definition}
    \textbf{Within-class Scatterness:} \\
    For embedding $\w^{T} \x$, the \textbf{embedding centre} for class k is:
    \begin{equation}
        \muhatk = \frac{1}{n_{k}} \sum_{i, y_{i}=k} \w^{T} \xui
    \end{equation}
    then the within-class scatterness of class k is:
    \begin{equation}
        s_{\w,k} := \sum_{i,y_{i}=k} (\w^{T} \xui - \muhatk)^{2}
    \end{equation}
\end{definition}
\begin{definition}
    \textbf{Between-class Scatterness:} \\
    The \textbf{embedded dataset centre} is:
    \begin{equation}
        \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} \w^{T} \xui
    \end{equation}
    then teh between-class scatterness is:
    \begin{equation}
        s_{b,k} = n_{k} (\muhatk - \hat{\mu})^{2}
    \end{equation}
    note the $n_{k}$ is needed to make $s_{b,k}$ the same scale as $s_{w,k}$.
\end{definition}
Then ideally we would like to maximize the between-class scatterness and minimize the within-class scatterness for all the classes, which is what the following does:
\begin{definition}
    \textbf{Fisher Discriminant Analysis (FDA):} \\
    \begin{equation}
        \max_{\w} [\sum_{k} s_{b,k} / \sum_{k} s_{\w,k} ]
    \end{equation}
\end{definition}
if $K=2$ then this has a simple solution: $\w := \bm{S}_{\w}^{-1} (\bm{\mu}_{+} - \bm{\mu}_{-})$, where $\bm{S}_{\w} := \sum_{k=1}^{K} \bm{S}_{k}$ and $\bm{S}_{k}$ is the sample covariance matrix of class k times $n_{k}$. However, note that the FDA does not learn a decision function f, the $\w_{FDA}$ obtained cannot be used directly by the prediction function to make a prediction. This is because (eg. in the binary case) $f(\x;\w_{FDA})>0$ does not mean that $\x$ is predicted as the positive class. The FDA also doesn't care about classification accuracy, ie. minimizing the FP or FN rate.

\subsection{Probabilistic (Generative) Classifiers}
