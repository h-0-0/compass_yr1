\section{Support Vector Machines}\label{section:Support-Vector-Machines}
In binary classification it might be that there are many decision boundaries we could draw that would correctly classify all the training data, this leaves us asking: what is the "optimal" decision boundary in binary classification? For our decision boundary to be generalizable we want to minimize the error on unseen datasets rather than the training set. We would like a decision boundary that will correctly classify a small perturbation of one of our training data points whilst still correctly classifying all of our training data, ie. we want to maximize the gap (the \textbf{margin}) between the decision boundary and the lines given by $f(\x;\w) = 1$ and $f(\x;\w) = -1$, whilst keeping data points on the correct side of the margin. 

If we are using decision boundary $f(\x;\w) := \langle\w_{1},\x\rangle + w_{0}$ then we can calculate the thickness of our margin as $\frac{1}{||\w_{1}||}$. Then we can formulate the following constrained minimization problem: minimize $||\w_{1}||^{2}$ subject to $\forall i, y_{i} \cdot f(\xui;\w) \geq 1$.

However, in many cases the dataset is not separable and so it is not possible to satisfy our constraint. We can relax these constraints which is what we do in the \textbf{soft-margin classifier}: $\min_{\w, \bm{\epsilon}} ||\w_{1}||^{2} + \sum_{i} \epsilon^{(i)}$ subject to $\forall i , \: y_{i}(\langle\w_{1},\xui \rangle + w_{0}) + \epsilon^{(i)} \geq 1, \: \epsilon^{(i)} \geq 0$. It turns out that this classifier is actually a convex minimization problem and so every local minimum is a global minimum.

We can use the lagrangian dual (discussed in \cref{Proofs:lagrangian-dual}) to turn the minimization problem of the soft-margin classifier into: $\max_{\bm{\lambda}} - \frac{\tilde{\bm{\lambda}}^{T} \X^{T} \X \tilde{\bm{\lambda}}}{4} + \langle\bm{\lambda},\bm{1} \rangle$ subject to $0 \leq \lambda_{i} \leq 1, \: \sum_{i} \lambda_{i} y_{i} =0$. Note that the constraints here are simpler and that its quadratic wrt. $\bm{\lambda} \in \mathbb{R}^{n}$ as opposed to quadratic wrt. $\w \in \mathbb{R}^{d+1}$ in our previous optimization problem. Hence, the lagrangian dual is slow when n is large and our previous problem is slow when d is large, also note that it is possible to use kernel methods in solving the lagrangian!

Notice that the border of the (optimal) margins will always pass through some data points, these points can be thought of as resisting the expansion of the margin, hence are called \textbf{support vectors}. Hence the name of this method: "Support Vector Machines" (SVM). However, the SVM has some limitations, for example it is important to note the SVM is not a probabilistic classifier as it lacks a probabilistic interpretation. Also note that the computational cost of SVM is high as it involves solving a constrained optimization problem (as opposed to unconstrained). Finally multi-class SVM is non-trivial as SVM is motivated by the geometry of binary classification. 