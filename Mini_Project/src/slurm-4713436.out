cpu-bind=MASK - bp1-gpu030, task  0  0 [25382]: mask 0x1000100 set
bp1-gpu030.data.bp.acrc.priv
JOB ID: 4713436
Working Directory: /user/work/jd18380/compass_yr1/Mini_Project/src
Start Time: Wed 22 Mar 16:28:47 GMT 2023

 
 
---------------------------- New Experiment ----------------------------
Data: CIFAR100
Model: FC_FF_NN
Batch size: 64
Learning rate: 0.01
Epochs: 100
Load model: False
Save model: True
Using cuda device
Files already downloaded and verified
Files already downloaded and verified
Epoch 1
-------------------------------
loss: 4.606913  [   64/50000]
loss: 4.593280  [ 6464/50000]
loss: 4.617553  [12864/50000]
loss: 4.625906  [19264/50000]
loss: 4.609357  [25664/50000]
loss: 4.595601  [32064/50000]
loss: 4.606676  [38464/50000]
loss: 4.602797  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.603891 

Epoch 2
-------------------------------
loss: 4.607141  [   64/50000]
loss: 4.609583  [ 6464/50000]
loss: 4.604070  [12864/50000]
loss: 4.597755  [19264/50000]
loss: 4.602180  [25664/50000]
loss: 4.611108  [32064/50000]
loss: 4.595912  [38464/50000]
loss: 4.608192  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.599701 

Epoch 3
-------------------------------
loss: 4.610093  [   64/50000]
loss: 4.600016  [ 6464/50000]
loss: 4.598083  [12864/50000]
loss: 4.601204  [19264/50000]
loss: 4.598568  [25664/50000]
loss: 4.592991  [32064/50000]
loss: 4.597049  [38464/50000]
loss: 4.578694  [44864/50000]
Test Error: 
 Accuracy: 1.3%, Avg loss: 4.590329 

Epoch 4
-------------------------------
loss: 4.595509  [   64/50000]
loss: 4.587167  [ 6464/50000]
loss: 4.585568  [12864/50000]
loss: 4.577813  [19264/50000]
loss: 4.583838  [25664/50000]
loss: 4.568543  [32064/50000]
loss: 4.552424  [38464/50000]
loss: 4.576965  [44864/50000]
Test Error: 
 Accuracy: 1.8%, Avg loss: 4.560120 

Epoch 5
-------------------------------
loss: 4.552536  [   64/50000]
loss: 4.523870  [ 6464/50000]
loss: 4.504466  [12864/50000]
loss: 4.572257  [19264/50000]
loss: 4.474018  [25664/50000]
loss: 4.511131  [32064/50000]
loss: 4.446013  [38464/50000]
loss: 4.355762  [44864/50000]
Test Error: 
 Accuracy: 3.9%, Avg loss: 4.365947 

Epoch 6
-------------------------------
loss: 4.384045  [   64/50000]
loss: 4.319008  [ 6464/50000]
loss: 4.355570  [12864/50000]
loss: 4.268940  [19264/50000]
loss: 4.380735  [25664/50000]
loss: 4.130527  [32064/50000]
loss: 4.222887  [38464/50000]
loss: 4.127799  [44864/50000]
Test Error: 
 Accuracy: 5.1%, Avg loss: 4.173657 

Epoch 7
-------------------------------
loss: 4.163318  [   64/50000]
loss: 4.227200  [ 6464/50000]
loss: 4.065354  [12864/50000]
loss: 4.277455  [19264/50000]
loss: 4.279254  [25664/50000]
loss: 4.012669  [32064/50000]
loss: 4.062644  [38464/50000]
loss: 4.103007  [44864/50000]
Test Error: 
 Accuracy: 6.3%, Avg loss: 4.094039 

Epoch 8
-------------------------------
loss: 4.093352  [   64/50000]
loss: 4.031570  [ 6464/50000]
loss: 4.023871  [12864/50000]
loss: 4.047312  [19264/50000]
loss: 4.142385  [25664/50000]
loss: 3.952928  [32064/50000]
loss: 4.081319  [38464/50000]
loss: 4.053245  [44864/50000]
Test Error: 
 Accuracy: 6.8%, Avg loss: 4.053806 

Epoch 9
-------------------------------
loss: 3.931748  [   64/50000]
loss: 3.711547  [ 6464/50000]
loss: 3.779658  [12864/50000]
loss: 4.039994  [19264/50000]
loss: 4.108175  [25664/50000]
loss: 3.837643  [32064/50000]
loss: 3.862595  [38464/50000]
loss: 3.941403  [44864/50000]
Test Error: 
 Accuracy: 6.7%, Avg loss: 4.071373 

Epoch 10
-------------------------------
loss: 3.766936  [   64/50000]
loss: 4.018525  [ 6464/50000]
loss: 3.813006  [12864/50000]
loss: 3.953147  [19264/50000]
loss: 3.927580  [25664/50000]
loss: 3.801888  [32064/50000]
loss: 3.997292  [38464/50000]
loss: 4.135564  [44864/50000]
Test Error: 
 Accuracy: 7.2%, Avg loss: 4.012580 

Epoch 11
-------------------------------
loss: 3.949843  [   64/50000]
loss: 4.005762  [ 6464/50000]
loss: 3.818400  [12864/50000]
loss: 3.937446  [19264/50000]
loss: 3.823449  [25664/50000]
loss: 3.971563  [32064/50000]
loss: 3.820561  [38464/50000]
loss: 3.970296  [44864/50000]
Test Error: 
 Accuracy: 8.0%, Avg loss: 3.981440 

Epoch 12
-------------------------------
loss: 3.557930  [   64/50000]
loss: 4.072974  [ 6464/50000]
loss: 3.963285  [12864/50000]
loss: 4.016788  [19264/50000]
loss: 4.039182  [25664/50000]
loss: 3.904959  [32064/50000]
loss: 3.945756  [38464/50000]
loss: 4.021781  [44864/50000]
Test Error: 
 Accuracy: 8.5%, Avg loss: 3.971070 

Epoch 13
-------------------------------
loss: 3.965986  [   64/50000]
loss: 3.996985  [ 6464/50000]
loss: 4.179375  [12864/50000]
loss: 3.937852  [19264/50000]
loss: 3.945908  [25664/50000]
loss: 3.885984  [32064/50000]
loss: 3.948671  [38464/50000]
loss: 3.919448  [44864/50000]
Test Error: 
 Accuracy: 8.5%, Avg loss: 3.968362 

Epoch 14
-------------------------------
loss: 3.780660  [   64/50000]
loss: 3.859370  [ 6464/50000]
loss: 3.883334  [12864/50000]
loss: 4.087455  [19264/50000]
loss: 3.866358  [25664/50000]
loss: 3.808511  [32064/50000]
loss: 4.080191  [38464/50000]
loss: 3.702431  [44864/50000]
Test Error: 
 Accuracy: 11.3%, Avg loss: 3.841508 

Epoch 15
-------------------------------
loss: 3.791380  [   64/50000]
loss: 3.798208  [ 6464/50000]
loss: 3.437825  [12864/50000]
loss: 3.602395  [19264/50000]
loss: 3.569389  [25664/50000]
loss: 4.013846  [32064/50000]
loss: 3.904879  [38464/50000]
loss: 4.024377  [44864/50000]
Test Error: 
 Accuracy: 11.8%, Avg loss: 3.804593 

Epoch 16
-------------------------------
loss: 3.914331  [   64/50000]
loss: 3.739358  [ 6464/50000]
loss: 3.737525  [12864/50000]
loss: 3.647637  [19264/50000]
loss: 3.864027  [25664/50000]
loss: 3.619257  [32064/50000]
loss: 3.773875  [38464/50000]
loss: 3.871400  [44864/50000]
Test Error: 
 Accuracy: 12.5%, Avg loss: 3.766728 

Epoch 17
-------------------------------
loss: 3.831942  [   64/50000]
loss: 3.381648  [ 6464/50000]
loss: 3.721047  [12864/50000]
loss: 3.399960  [19264/50000]
loss: 3.939874  [25664/50000]
loss: 3.690077  [32064/50000]
loss: 3.749037  [38464/50000]
loss: 3.627704  [44864/50000]
Test Error: 
 Accuracy: 13.8%, Avg loss: 3.714241 

Epoch 18
-------------------------------
loss: 3.566947  [   64/50000]
loss: 3.736818  [ 6464/50000]
loss: 3.842103  [12864/50000]
loss: 3.494135  [19264/50000]
loss: 3.692849  [25664/50000]
loss: 3.562096  [32064/50000]
loss: 3.870133  [38464/50000]
loss: 3.487419  [44864/50000]
Test Error: 
 Accuracy: 12.8%, Avg loss: 3.746720 

Epoch 19
-------------------------------
loss: 3.918316  [   64/50000]
loss: 3.685473  [ 6464/50000]
loss: 3.506499  [12864/50000]
loss: 3.706946  [19264/50000]
loss: 3.169455  [25664/50000]
loss: 3.814850  [32064/50000]
loss: 3.779266  [38464/50000]
loss: 3.567244  [44864/50000]
Test Error: 
 Accuracy: 13.5%, Avg loss: 3.686428 

Epoch 20
-------------------------------
loss: 3.636141  [   64/50000]
loss: 3.441784  [ 6464/50000]
loss: 3.793154  [12864/50000]
loss: 3.555321  [19264/50000]
loss: 3.966928  [25664/50000]
loss: 3.781681  [32064/50000]
loss: 3.717015  [38464/50000]
loss: 3.676653  [44864/50000]
Test Error: 
 Accuracy: 13.1%, Avg loss: 3.713861 

Epoch 21
-------------------------------
loss: 3.648786  [   64/50000]
loss: 3.546578  [ 6464/50000]
loss: 3.843229  [12864/50000]
loss: 3.545133  [19264/50000]
loss: 3.868029  [25664/50000]
loss: 3.452133  [32064/50000]
loss: 3.499095  [38464/50000]
loss: 3.829094  [44864/50000]
Test Error: 
 Accuracy: 14.1%, Avg loss: 3.680912 

Epoch 22
-------------------------------
loss: 4.186890  [   64/50000]
loss: 3.501892  [ 6464/50000]
loss: 3.855504  [12864/50000]
loss: 3.377455  [19264/50000]
loss: 3.412221  [25664/50000]
loss: 3.514902  [32064/50000]
loss: 3.263569  [38464/50000]
loss: 3.589451  [44864/50000]
Test Error: 
 Accuracy: 15.9%, Avg loss: 3.597480 

Epoch 23
-------------------------------
loss: 3.493395  [   64/50000]
loss: 3.431925  [ 6464/50000]
loss: 3.491689  [12864/50000]
loss: 3.567233  [19264/50000]
loss: 3.579976  [25664/50000]
loss: 3.451283  [32064/50000]
loss: 3.471711  [38464/50000]
loss: 3.585366  [44864/50000]
Test Error: 
 Accuracy: 14.2%, Avg loss: 3.642233 

Epoch 24
-------------------------------
loss: 3.799046  [   64/50000]
loss: 3.508936  [ 6464/50000]
loss: 3.454604  [12864/50000]
loss: 3.386024  [19264/50000]
loss: 3.401320  [25664/50000]
loss: 3.709861  [32064/50000]
loss: 3.413224  [38464/50000]
loss: 3.427553  [44864/50000]
Test Error: 
 Accuracy: 14.9%, Avg loss: 3.604627 

Epoch 25
-------------------------------
loss: 3.331637  [   64/50000]
loss: 3.739060  [ 6464/50000]
loss: 3.633729  [12864/50000]
loss: 3.333398  [19264/50000]
loss: 3.485917  [25664/50000]
loss: 3.342623  [32064/50000]
loss: 3.597778  [38464/50000]
loss: 3.438323  [44864/50000]
Test Error: 
 Accuracy: 16.4%, Avg loss: 3.559231 

Epoch 26
-------------------------------
loss: 3.529716  [   64/50000]
loss: 3.943339  [ 6464/50000]
loss: 3.365184  [12864/50000]
loss: 3.304683  [19264/50000]
loss: 3.771306  [25664/50000]
loss: 3.348161  [32064/50000]
loss: 3.493390  [38464/50000]
loss: 3.307024  [44864/50000]
Test Error: 
 Accuracy: 16.1%, Avg loss: 3.558515 

Epoch 27
-------------------------------
loss: 3.114351  [   64/50000]
loss: 3.203025  [ 6464/50000]
loss: 3.610005  [12864/50000]
loss: 3.164926  [19264/50000]
loss: 3.564677  [25664/50000]
loss: 3.253374  [32064/50000]
loss: 3.641592  [38464/50000]
loss: 3.457650  [44864/50000]
Test Error: 
 Accuracy: 16.0%, Avg loss: 3.576013 

Epoch 28
-------------------------------
loss: 3.841661  [   64/50000]
loss: 3.272220  [ 6464/50000]
loss: 3.220260  [12864/50000]
loss: 3.179381  [19264/50000]
loss: 3.485317  [25664/50000]
loss: 3.702054  [32064/50000]
loss: 3.495830  [38464/50000]
loss: 3.223851  [44864/50000]
Test Error: 
 Accuracy: 15.6%, Avg loss: 3.569609 

Epoch 29
-------------------------------
loss: 3.608467  [   64/50000]
loss: 3.474061  [ 6464/50000]
loss: 3.427432  [12864/50000]
loss: 3.192090  [19264/50000]
loss: 3.116027  [25664/50000]
loss: 3.358239  [32064/50000]
loss: 3.379372  [38464/50000]
loss: 3.246069  [44864/50000]
Test Error: 
 Accuracy: 17.2%, Avg loss: 3.491784 

Epoch 30
-------------------------------
loss: 3.383534  [   64/50000]
loss: 3.376099  [ 6464/50000]
loss: 3.562524  [12864/50000]
loss: 3.388551  [19264/50000]
loss: 3.794003  [25664/50000]
loss: 3.521769  [32064/50000]
loss: 3.341722  [38464/50000]
loss: 3.196233  [44864/50000]
Test Error: 
 Accuracy: 16.9%, Avg loss: 3.518413 

Epoch 31
-------------------------------
loss: 3.243809  [   64/50000]
loss: 3.081766  [ 6464/50000]
loss: 3.393721  [12864/50000]
loss: 3.236592  [19264/50000]
loss: 3.361600  [25664/50000]
loss: 3.189869  [32064/50000]
loss: 3.579593  [38464/50000]
loss: 3.347828  [44864/50000]
Test Error: 
 Accuracy: 15.2%, Avg loss: 3.633687 

Epoch 32
-------------------------------
loss: 3.568466  [   64/50000]
loss: 3.508234  [ 6464/50000]
loss: 3.283873  [12864/50000]
loss: 3.207900  [19264/50000]
loss: 3.627398  [25664/50000]
loss: 3.281600  [32064/50000]
loss: 3.246594  [38464/50000]
loss: 3.282713  [44864/50000]
Test Error: 
 Accuracy: 16.3%, Avg loss: 3.545348 

Epoch 33
-------------------------------
loss: 3.396238  [   64/50000]
loss: 3.302089  [ 6464/50000]
loss: 3.514051  [12864/50000]
loss: 3.410022  [19264/50000]
loss: 3.359672  [25664/50000]
loss: 3.291288  [32064/50000]
loss: 3.580829  [38464/50000]
loss: 3.214517  [44864/50000]
Test Error: 
 Accuracy: 18.7%, Avg loss: 3.413282 

Epoch 34
-------------------------------
loss: 3.247740  [   64/50000]
loss: 3.250191  [ 6464/50000]
loss: 3.181057  [12864/50000]
loss: 3.063677  [19264/50000]
loss: 3.450731  [25664/50000]
loss: 3.224481  [32064/50000]
loss: 3.234862  [38464/50000]
loss: 3.289591  [44864/50000]
Test Error: 
 Accuracy: 15.7%, Avg loss: 3.702586 

Epoch 35
-------------------------------
loss: 3.823775  [   64/50000]
loss: 3.601895  [ 6464/50000]
loss: 3.201594  [12864/50000]
loss: 3.401981  [19264/50000]
loss: 3.153375  [25664/50000]
loss: 3.152494  [32064/50000]
loss: 3.009545  [38464/50000]
loss: 3.512269  [44864/50000]
Test Error: 
 Accuracy: 18.3%, Avg loss: 3.454691 

Epoch 36
-------------------------------
loss: 3.426757  [   64/50000]
loss: 3.545438  [ 6464/50000]
loss: 3.234145  [12864/50000]
loss: 2.848223  [19264/50000]
loss: 3.378621  [25664/50000]
loss: 3.295566  [32064/50000]
loss: 3.389814  [38464/50000]
loss: 3.389370  [44864/50000]
Test Error: 
 Accuracy: 18.8%, Avg loss: 3.418286 

Epoch 37
-------------------------------
loss: 3.017658  [   64/50000]
loss: 3.632030  [ 6464/50000]
loss: 3.273515  [12864/50000]
loss: 3.044713  [19264/50000]
loss: 3.679240  [25664/50000]
loss: 3.181420  [32064/50000]
loss: 2.976939  [38464/50000]
loss: 3.300343  [44864/50000]
Test Error: 
 Accuracy: 17.9%, Avg loss: 3.445594 

Epoch 38
-------------------------------
loss: 3.210962  [   64/50000]
loss: 3.342490  [ 6464/50000]
loss: 2.944288  [12864/50000]
loss: 3.125436  [19264/50000]
loss: 3.464931  [25664/50000]
loss: 3.248804  [32064/50000]
loss: 3.087984  [38464/50000]
loss: 3.221307  [44864/50000]
Test Error: 
 Accuracy: 18.2%, Avg loss: 3.481481 

Epoch 39
-------------------------------
loss: 3.529806  [   64/50000]
loss: 3.276023  [ 6464/50000]
loss: 2.857826  [12864/50000]
loss: 3.184579  [19264/50000]
loss: 2.991322  [25664/50000]
loss: 3.381282  [32064/50000]
loss: 3.237689  [38464/50000]
loss: 3.300817  [44864/50000]
Test Error: 
 Accuracy: 18.4%, Avg loss: 3.427646 

Epoch 40
-------------------------------
loss: 3.120635  [   64/50000]
loss: 3.111987  [ 6464/50000]
loss: 3.089407  [12864/50000]
loss: 3.294744  [19264/50000]
loss: 3.199916  [25664/50000]
loss: 3.143650  [32064/50000]
loss: 3.378263  [38464/50000]
loss: 3.145425  [44864/50000]
Test Error: 
 Accuracy: 19.2%, Avg loss: 3.422796 

Epoch 41
-------------------------------
loss: 3.534224  [   64/50000]
loss: 3.359103  [ 6464/50000]
loss: 3.128248  [12864/50000]
loss: 2.999787  [19264/50000]
loss: 3.198852  [25664/50000]
loss: 3.624658  [32064/50000]
loss: 3.214658  [38464/50000]
loss: 3.272121  [44864/50000]
Test Error: 
 Accuracy: 16.1%, Avg loss: 3.601137 

Epoch 42
-------------------------------
loss: 3.619884  [   64/50000]
loss: 2.987530  [ 6464/50000]
loss: 3.372030  [12864/50000]
loss: 3.200310  [19264/50000]
loss: 3.155789  [25664/50000]
loss: 3.360070  [32064/50000]
loss: 3.365356  [38464/50000]
loss: 3.279471  [44864/50000]
Test Error: 
 Accuracy: 18.8%, Avg loss: 3.435798 

Epoch 43
-------------------------------
loss: 3.654526  [   64/50000]
loss: 3.192567  [ 6464/50000]
loss: 2.785612  [12864/50000]
loss: 3.235997  [19264/50000]
loss: 3.235574  [25664/50000]
loss: 3.135598  [32064/50000]
loss: 3.211358  [38464/50000]
loss: 3.498895  [44864/50000]
Test Error: 
 Accuracy: 16.6%, Avg loss: 3.579419 

Epoch 44
-------------------------------
loss: 3.351312  [   64/50000]
loss: 2.952574  [ 6464/50000]
loss: 3.247511  [12864/50000]
loss: 3.101866  [19264/50000]
loss: 2.809048  [25664/50000]
loss: 2.927292  [32064/50000]
loss: 3.069379  [38464/50000]
loss: 2.980216  [44864/50000]
Test Error: 
 Accuracy: 20.0%, Avg loss: 3.359691 

Epoch 45
-------------------------------
loss: 3.182343  [   64/50000]
loss: 3.007236  [ 6464/50000]
loss: 3.010403  [12864/50000]
loss: 3.310416  [19264/50000]
loss: 3.134894  [25664/50000]
loss: 3.272273  [32064/50000]
loss: 2.967247  [38464/50000]
loss: 3.173303  [44864/50000]
Test Error: 
 Accuracy: 19.8%, Avg loss: 3.356533 

Epoch 46
-------------------------------
loss: 3.455577  [   64/50000]
loss: 3.139768  [ 6464/50000]
loss: 3.297601  [12864/50000]
loss: 3.286245  [19264/50000]
loss: 3.357485  [25664/50000]
loss: 3.371535  [32064/50000]
loss: 3.369645  [38464/50000]
loss: 3.200337  [44864/50000]
Test Error: 
 Accuracy: 19.6%, Avg loss: 3.381589 

Epoch 47
-------------------------------
loss: 2.930931  [   64/50000]
loss: 2.989371  [ 6464/50000]
loss: 3.438024  [12864/50000]
loss: 3.274376  [19264/50000]
loss: 2.971938  [25664/50000]
loss: 2.968159  [32064/50000]
loss: 3.520884  [38464/50000]
loss: 3.057590  [44864/50000]
Test Error: 
 Accuracy: 19.8%, Avg loss: 3.360487 

Epoch 48
-------------------------------
loss: 3.379267  [   64/50000]
loss: 2.949770  [ 6464/50000]
loss: 3.201066  [12864/50000]
loss: 3.323886  [19264/50000]
loss: 2.981872  [25664/50000]
loss: 3.393368  [32064/50000]
loss: 2.720383  [38464/50000]
loss: 3.368659  [44864/50000]
Test Error: 
 Accuracy: 17.9%, Avg loss: 3.476658 

Epoch 49
-------------------------------
loss: 3.052583  [   64/50000]
loss: 3.303138  [ 6464/50000]
loss: 3.159679  [12864/50000]
loss: 2.985248  [19264/50000]
loss: 2.967059  [25664/50000]
loss: 3.334253  [32064/50000]
loss: 3.143680  [38464/50000]
loss: 3.391699  [44864/50000]
Test Error: 
 Accuracy: 19.9%, Avg loss: 3.367541 

Epoch 50
-------------------------------
loss: 3.716229  [   64/50000]
loss: 2.911295  [ 6464/50000]
loss: 3.091929  [12864/50000]
loss: 3.226058  [19264/50000]
loss: 3.235572  [25664/50000]
loss: 2.674906  [32064/50000]
loss: 2.951438  [38464/50000]
loss: 2.994065  [44864/50000]
Test Error: 
 Accuracy: 18.6%, Avg loss: 3.453976 

Epoch 51
-------------------------------
loss: 3.827254  [   64/50000]
loss: 2.790476  [ 6464/50000]
loss: 3.120275  [12864/50000]
loss: 3.376954  [19264/50000]
loss: 3.188850  [25664/50000]
loss: 3.169617  [32064/50000]
loss: 3.048545  [38464/50000]
loss: 3.186818  [44864/50000]
Test Error: 
 Accuracy: 21.4%, Avg loss: 3.309411 

Epoch 52
-------------------------------
loss: 3.112301  [   64/50000]
loss: 3.111008  [ 6464/50000]
loss: 2.902782  [12864/50000]
loss: 3.027432  [19264/50000]
loss: 3.070030  [25664/50000]
loss: 3.473611  [32064/50000]
loss: 2.914625  [38464/50000]
loss: 3.084061  [44864/50000]
Test Error: 
 Accuracy: 20.4%, Avg loss: 3.349760 

Epoch 53
-------------------------------
loss: 3.387183  [   64/50000]
loss: 3.128419  [ 6464/50000]
loss: 3.554253  [12864/50000]
loss: 2.922069  [19264/50000]
loss: 3.272681  [25664/50000]
loss: 2.759532  [32064/50000]
loss: 3.287123  [38464/50000]
loss: 3.148799  [44864/50000]
Test Error: 
 Accuracy: 18.2%, Avg loss: 3.506393 

Epoch 54
-------------------------------
loss: 3.379599  [   64/50000]
loss: 3.224474  [ 6464/50000]
loss: 2.721332  [12864/50000]
loss: 3.138762  [19264/50000]
loss: 2.725179  [25664/50000]
loss: 3.004559  [32064/50000]
loss: 3.101562  [38464/50000]
loss: 3.272219  [44864/50000]
Test Error: 
 Accuracy: 20.8%, Avg loss: 3.297312 

Epoch 55
-------------------------------
loss: 3.275667  [   64/50000]
loss: 3.203942  [ 6464/50000]
loss: 2.830258  [12864/50000]
loss: 2.953549  [19264/50000]
loss: 2.939744  [25664/50000]
loss: 3.277624  [32064/50000]
loss: 3.072614  [38464/50000]
loss: 2.540163  [44864/50000]
Test Error: 
 Accuracy: 19.8%, Avg loss: 3.381318 

Epoch 56
-------------------------------
loss: 2.960113  [   64/50000]
loss: 2.767558  [ 6464/50000]
loss: 3.130539  [12864/50000]
loss: 3.381642  [19264/50000]
loss: 3.032564  [25664/50000]
loss: 2.820872  [32064/50000]
loss: 2.808378  [38464/50000]
loss: 2.942968  [44864/50000]
Test Error: 
 Accuracy: 20.6%, Avg loss: 3.364551 

Epoch 57
-------------------------------
loss: 3.277418  [   64/50000]
loss: 3.257000  [ 6464/50000]
loss: 2.893914  [12864/50000]
loss: 3.089887  [19264/50000]
loss: 3.359243  [25664/50000]
loss: 3.069376  [32064/50000]
loss: 2.947539  [38464/50000]
loss: 2.875810  [44864/50000]
Test Error: 
 Accuracy: 20.0%, Avg loss: 3.378832 

Epoch 58
-------------------------------
loss: 3.386846  [   64/50000]
loss: 2.805390  [ 6464/50000]
loss: 2.808075  [12864/50000]
loss: 3.188047  [19264/50000]
loss: 2.698416  [25664/50000]
loss: 3.216020  [32064/50000]
loss: 2.737221  [38464/50000]
loss: 2.946572  [44864/50000]
Test Error: 
 Accuracy: 21.6%, Avg loss: 3.288812 

Epoch 59
-------------------------------
loss: 2.792019  [   64/50000]
loss: 3.054625  [ 6464/50000]
loss: 2.993560  [12864/50000]
loss: 3.011795  [19264/50000]
loss: 2.766660  [25664/50000]
loss: 3.170715  [32064/50000]
loss: 2.959515  [38464/50000]
loss: 2.784525  [44864/50000]
Test Error: 
 Accuracy: 19.0%, Avg loss: 3.428597 

Epoch 60
-------------------------------
loss: 3.003659  [   64/50000]
loss: 2.936029  [ 6464/50000]
loss: 2.999839  [12864/50000]
loss: 3.212735  [19264/50000]
loss: 2.849866  [25664/50000]
loss: 3.165225  [32064/50000]
loss: 2.682517  [38464/50000]
loss: 3.003895  [44864/50000]
Test Error: 
 Accuracy: 21.3%, Avg loss: 3.314172 

Epoch 61
-------------------------------
loss: 3.472349  [   64/50000]
loss: 2.985974  [ 6464/50000]
loss: 3.121125  [12864/50000]
loss: 3.073823  [19264/50000]
loss: 2.927784  [25664/50000]
loss: 3.060292  [32064/50000]
loss: 2.613456  [38464/50000]
loss: 3.035268  [44864/50000]
Test Error: 
 Accuracy: 23.0%, Avg loss: 3.225824 

Epoch 62
-------------------------------
loss: 3.125279  [   64/50000]
loss: 2.984087  [ 6464/50000]
loss: 3.159717  [12864/50000]
loss: 2.867221  [19264/50000]
loss: 2.712927  [25664/50000]
loss: 2.597344  [32064/50000]
loss: 2.932197  [38464/50000]
loss: 3.022934  [44864/50000]
Test Error: 
 Accuracy: 22.2%, Avg loss: 3.250668 

Epoch 63
-------------------------------
loss: 2.788397  [   64/50000]
loss: 3.097245  [ 6464/50000]
loss: 2.786348  [12864/50000]
loss: 3.028004  [19264/50000]
loss: 2.643227  [25664/50000]
loss: 3.002616  [32064/50000]
loss: 2.747127  [38464/50000]
loss: 2.698768  [44864/50000]
Test Error: 
 Accuracy: 19.8%, Avg loss: 3.442097 

Epoch 64
-------------------------------
loss: 2.855877  [   64/50000]
loss: 2.627361  [ 6464/50000]
loss: 2.896354  [12864/50000]
loss: 2.818371  [19264/50000]
loss: 2.755152  [25664/50000]
loss: 3.041928  [32064/50000]
loss: 2.848015  [38464/50000]
loss: 2.958793  [44864/50000]
Test Error: 
 Accuracy: 21.9%, Avg loss: 3.274641 

Epoch 65
-------------------------------
loss: 2.725101  [   64/50000]
loss: 2.614012  [ 6464/50000]
loss: 2.764479  [12864/50000]
loss: 2.795623  [19264/50000]
loss: 2.429310  [25664/50000]
loss: 2.863903  [32064/50000]
loss: 3.055337  [38464/50000]
loss: 2.715620  [44864/50000]
Test Error: 
 Accuracy: 19.8%, Avg loss: 3.422515 

Epoch 66
-------------------------------
loss: 3.146377  [   64/50000]
loss: 2.827308  [ 6464/50000]
loss: 2.909016  [12864/50000]
loss: 2.938499  [19264/50000]
loss: 2.991707  [25664/50000]
loss: 3.042204  [32064/50000]
loss: 2.847133  [38464/50000]
loss: 3.092962  [44864/50000]
Test Error: 
 Accuracy: 21.1%, Avg loss: 3.348543 

Epoch 67
-------------------------------
loss: 2.835441  [   64/50000]
loss: 2.704378  [ 6464/50000]
loss: 2.703959  [12864/50000]
loss: 2.819193  [19264/50000]
loss: 2.917431  [25664/50000]
loss: 2.985501  [32064/50000]
loss: 2.999795  [38464/50000]
loss: 3.055682  [44864/50000]
Test Error: 
 Accuracy: 22.8%, Avg loss: 3.204164 

Epoch 68
-------------------------------
loss: 2.848871  [   64/50000]
loss: 2.946188  [ 6464/50000]
loss: 3.027588  [12864/50000]
loss: 3.187621  [19264/50000]
loss: 2.812207  [25664/50000]
loss: 3.045154  [32064/50000]
loss: 2.843862  [38464/50000]
loss: 2.927247  [44864/50000]
Test Error: 
 Accuracy: 22.9%, Avg loss: 3.228019 

Epoch 69
-------------------------------
loss: 3.157936  [   64/50000]
loss: 2.669080  [ 6464/50000]
loss: 2.812450  [12864/50000]
loss: 2.869915  [19264/50000]
loss: 3.120990  [25664/50000]
loss: 2.647472  [32064/50000]
loss: 2.637966  [38464/50000]
loss: 2.576173  [44864/50000]
Test Error: 
 Accuracy: 21.0%, Avg loss: 3.394439 

Epoch 70
-------------------------------
loss: 3.521376  [   64/50000]
loss: 2.615449  [ 6464/50000]
loss: 2.624958  [12864/50000]
loss: 2.864218  [19264/50000]
loss: 2.625833  [25664/50000]
loss: 3.240701  [32064/50000]
loss: 2.885874  [38464/50000]
loss: 3.038989  [44864/50000]
Test Error: 
 Accuracy: 23.4%, Avg loss: 3.219547 

Epoch 71
-------------------------------
loss: 2.459310  [   64/50000]
loss: 2.522312  [ 6464/50000]
loss: 2.820462  [12864/50000]
loss: 2.741495  [19264/50000]
loss: 2.815374  [25664/50000]
loss: 2.614636  [32064/50000]
loss: 2.747313  [38464/50000]
loss: 2.661160  [44864/50000]
Test Error: 
 Accuracy: 23.6%, Avg loss: 3.205582 

Epoch 72
-------------------------------
loss: 2.957559  [   64/50000]
loss: 2.520110  [ 6464/50000]
loss: 2.806693  [12864/50000]
loss: 2.456690  [19264/50000]
loss: 2.822672  [25664/50000]
loss: 2.593233  [32064/50000]
loss: 3.266778  [38464/50000]
loss: 2.718643  [44864/50000]
Test Error: 
 Accuracy: 23.0%, Avg loss: 3.231543 

Epoch 73
-------------------------------
loss: 2.655889  [   64/50000]
loss: 3.159667  [ 6464/50000]
loss: 2.727548  [12864/50000]
loss: 2.674207  [19264/50000]
loss: 3.010396  [25664/50000]
loss: 2.890784  [32064/50000]
loss: 2.693849  [38464/50000]
loss: 2.727945  [44864/50000]
Test Error: 
 Accuracy: 21.4%, Avg loss: 3.321684 

Epoch 74
-------------------------------
loss: 2.862386  [   64/50000]
loss: 2.781541  [ 6464/50000]
loss: 2.727891  [12864/50000]
loss: 3.021466  [19264/50000]
loss: 2.679727  [25664/50000]
loss: 2.829210  [32064/50000]
loss: 2.494362  [38464/50000]
loss: 2.879229  [44864/50000]
Test Error: 
 Accuracy: 23.2%, Avg loss: 3.254422 

Epoch 75
-------------------------------
loss: 2.466636  [   64/50000]
loss: 2.713514  [ 6464/50000]
loss: 2.642488  [12864/50000]
loss: 2.685156  [19264/50000]
loss: 2.657557  [25664/50000]
loss: 2.792391  [32064/50000]
loss: 2.875073  [38464/50000]
loss: 2.732256  [44864/50000]
Test Error: 
 Accuracy: 23.2%, Avg loss: 3.239680 

Epoch 76
-------------------------------
loss: 2.941503  [   64/50000]
loss: 2.817785  [ 6464/50000]
loss: 2.612144  [12864/50000]
loss: 2.450645  [19264/50000]
loss: 2.890735  [25664/50000]
loss: 2.667389  [32064/50000]
loss: 2.712048  [38464/50000]
loss: 2.837566  [44864/50000]
Test Error: 
 Accuracy: 23.7%, Avg loss: 3.190879 

Epoch 77
-------------------------------
loss: 2.725335  [   64/50000]
loss: 2.995540  [ 6464/50000]
loss: 2.710205  [12864/50000]
loss: 2.482756  [19264/50000]
loss: 2.597194  [25664/50000]
loss: 2.557465  [32064/50000]
loss: 2.710450  [38464/50000]
loss: 2.504978  [44864/50000]
Test Error: 
 Accuracy: 21.9%, Avg loss: 3.323493 

Epoch 78
-------------------------------
loss: 2.812096  [   64/50000]
loss: 2.762444  [ 6464/50000]
loss: 2.961507  [12864/50000]
loss: 2.589304  [19264/50000]
loss: 2.864949  [25664/50000]
loss: 2.440324  [32064/50000]
loss: 2.583650  [38464/50000]
loss: 2.782745  [44864/50000]
Test Error: 
 Accuracy: 23.4%, Avg loss: 3.221335 

Epoch 79
-------------------------------
loss: 2.906078  [   64/50000]
loss: 2.636011  [ 6464/50000]
loss: 2.309460  [12864/50000]
loss: 2.558856  [19264/50000]
loss: 2.691625  [25664/50000]
loss: 2.898096  [32064/50000]
loss: 2.860442  [38464/50000]
loss: 2.720374  [44864/50000]
Test Error: 
 Accuracy: 22.7%, Avg loss: 3.300785 

Epoch 80
-------------------------------
loss: 2.522612  [   64/50000]
loss: 2.672681  [ 6464/50000]
loss: 2.305979  [12864/50000]
loss: 3.129724  [19264/50000]
loss: 2.672259  [25664/50000]
loss: 2.596124  [32064/50000]
loss: 2.752934  [38464/50000]
loss: 2.729922  [44864/50000]
Test Error: 
 Accuracy: 22.1%, Avg loss: 3.336285 

Epoch 81
-------------------------------
loss: 2.871136  [   64/50000]
loss: 2.750368  [ 6464/50000]
loss: 2.371734  [12864/50000]
loss: 2.729197  [19264/50000]
loss: 3.161399  [25664/50000]
loss: 2.768704  [32064/50000]
loss: 3.003163  [38464/50000]
loss: 2.895327  [44864/50000]
Test Error: 
 Accuracy: 21.6%, Avg loss: 3.350218 

Epoch 82
-------------------------------
loss: 2.731604  [   64/50000]
loss: 2.479611  [ 6464/50000]
loss: 2.759111  [12864/50000]
loss: 2.537314  [19264/50000]
loss: 2.255910  [25664/50000]
loss: 2.405014  [32064/50000]
loss: 2.564451  [38464/50000]
loss: 2.543308  [44864/50000]
Test Error: 
 Accuracy: 22.7%, Avg loss: 3.289835 

Epoch 83
-------------------------------
loss: 2.694319  [   64/50000]
loss: 2.840570  [ 6464/50000]
loss: 2.792017  [12864/50000]
loss: 2.512476  [19264/50000]
loss: 2.281886  [25664/50000]
loss: 2.698079  [32064/50000]
loss: 3.186607  [38464/50000]
loss: 2.691764  [44864/50000]
Test Error: 
 Accuracy: 24.8%, Avg loss: 3.159588 

Epoch 84
-------------------------------
loss: 2.763476  [   64/50000]
loss: 2.681754  [ 6464/50000]
loss: 2.618429  [12864/50000]
loss: 2.652073  [19264/50000]
loss: 2.472366  [25664/50000]
loss: 2.779844  [32064/50000]
loss: 2.466840  [38464/50000]
loss: 2.816241  [44864/50000]
Test Error: 
 Accuracy: 24.1%, Avg loss: 3.188752 

Epoch 85
-------------------------------
loss: 2.862847  [   64/50000]
loss: 2.571515  [ 6464/50000]
loss: 2.238580  [12864/50000]
loss: 2.610692  [19264/50000]
loss: 2.416234  [25664/50000]
loss: 2.669958  [32064/50000]
loss: 2.416693  [38464/50000]
loss: 2.461176  [44864/50000]
Test Error: 
 Accuracy: 23.2%, Avg loss: 3.317743 

Epoch 86
-------------------------------
loss: 2.921911  [   64/50000]
loss: 2.568997  [ 6464/50000]
loss: 2.688715  [12864/50000]
loss: 2.635006  [19264/50000]
loss: 2.348925  [25664/50000]
loss: 2.308614  [32064/50000]
loss: 2.548557  [38464/50000]
loss: 2.611562  [44864/50000]
Test Error: 
 Accuracy: 22.3%, Avg loss: 3.377718 

Epoch 87
-------------------------------
loss: 2.907506  [   64/50000]
loss: 2.786848  [ 6464/50000]
loss: 2.747456  [12864/50000]
loss: 2.552304  [19264/50000]
loss: 2.645077  [25664/50000]
loss: 2.562004  [32064/50000]
loss: 2.572031  [38464/50000]
loss: 2.393529  [44864/50000]
Test Error: 
 Accuracy: 23.8%, Avg loss: 3.216158 

Epoch 88
-------------------------------
loss: 2.696053  [   64/50000]
loss: 2.089278  [ 6464/50000]
loss: 2.580784  [12864/50000]
loss: 2.497318  [19264/50000]
loss: 2.342098  [25664/50000]
loss: 2.747179  [32064/50000]
loss: 2.621108  [38464/50000]
loss: 2.588716  [44864/50000]
Test Error: 
 Accuracy: 24.0%, Avg loss: 3.200443 

Epoch 89
-------------------------------
loss: 2.568542  [   64/50000]
loss: 2.517497  [ 6464/50000]
loss: 2.444569  [12864/50000]
loss: 2.419389  [19264/50000]
loss: 2.469144  [25664/50000]
loss: 2.317760  [32064/50000]
loss: 2.431782  [38464/50000]
loss: 2.893527  [44864/50000]
Test Error: 
 Accuracy: 22.7%, Avg loss: 3.339844 

Epoch 90
-------------------------------
loss: 3.089790  [   64/50000]
loss: 2.804441  [ 6464/50000]
loss: 2.560121  [12864/50000]
loss: 2.538553  [19264/50000]
loss: 2.454835  [25664/50000]
loss: 3.001696  [32064/50000]
loss: 3.123848  [38464/50000]
loss: 2.508085  [44864/50000]
Test Error: 
 Accuracy: 21.6%, Avg loss: 3.515050 

Epoch 91
-------------------------------
loss: 2.964005  [   64/50000]
loss: 2.503542  [ 6464/50000]
loss: 2.440204  [12864/50000]
loss: 2.497482  [19264/50000]
loss: 2.363883  [25664/50000]
loss: 2.399271  [32064/50000]
loss: 2.396495  [38464/50000]
loss: 2.387889  [44864/50000]
Test Error: 
 Accuracy: 23.9%, Avg loss: 3.232179 

Epoch 92
-------------------------------
loss: 2.740640  [   64/50000]
loss: 2.385808  [ 6464/50000]
loss: 2.333713  [12864/50000]
loss: 2.527333  [19264/50000]
loss: 2.521813  [25664/50000]
loss: 2.715967  [32064/50000]
loss: 2.428771  [38464/50000]
loss: 2.844140  [44864/50000]
Test Error: 
 Accuracy: 23.0%, Avg loss: 3.349089 

Epoch 93
-------------------------------
loss: 2.753369  [   64/50000]
loss: 2.227916  [ 6464/50000]
loss: 2.468420  [12864/50000]
loss: 2.400977  [19264/50000]
loss: 2.423789  [25664/50000]
loss: 2.570311  [32064/50000]
loss: 2.866798  [38464/50000]
loss: 2.631427  [44864/50000]
Test Error: 
 Accuracy: 24.3%, Avg loss: 3.216543 

Epoch 94
-------------------------------
loss: 2.848628  [   64/50000]
loss: 2.548862  [ 6464/50000]
loss: 2.500862  [12864/50000]
loss: 2.397002  [19264/50000]
loss: 2.141370  [25664/50000]
loss: 2.466841  [32064/50000]
loss: 2.520393  [38464/50000]
loss: 2.682385  [44864/50000]
Test Error: 
 Accuracy: 21.9%, Avg loss: 3.439823 

Epoch 95
-------------------------------
loss: 3.104948  [   64/50000]
loss: 2.621162  [ 6464/50000]
loss: 2.321161  [12864/50000]
loss: 2.183360  [19264/50000]
loss: 2.539626  [25664/50000]
loss: 2.650308  [32064/50000]
loss: 2.359737  [38464/50000]
loss: 2.608956  [44864/50000]
Test Error: 
 Accuracy: 24.9%, Avg loss: 3.198309 

Epoch 96
-------------------------------
loss: 2.876209  [   64/50000]
loss: 2.613245  [ 6464/50000]
loss: 2.715735  [12864/50000]
loss: 2.705614  [19264/50000]
loss: 2.378448  [25664/50000]
loss: 2.411089  [32064/50000]
loss: 2.599593  [38464/50000]
loss: 3.001269  [44864/50000]
Test Error: 
 Accuracy: 24.6%, Avg loss: 3.268819 

Epoch 97
-------------------------------
loss: 2.938231  [   64/50000]
loss: 2.164002  [ 6464/50000]
loss: 2.627645  [12864/50000]
loss: 2.339494  [19264/50000]
loss: 2.217779  [25664/50000]
loss: 2.648026  [32064/50000]
loss: 2.362996  [38464/50000]
loss: 2.849297  [44864/50000]
Test Error: 
 Accuracy: 22.7%, Avg loss: 3.356096 

Epoch 98
-------------------------------
loss: 2.784610  [   64/50000]
loss: 2.778352  [ 6464/50000]
loss: 2.464298  [12864/50000]
loss: 2.468246  [19264/50000]
loss: 2.499680  [25664/50000]
loss: 2.403784  [32064/50000]
loss: 2.086304  [38464/50000]
loss: 2.227669  [44864/50000]
Test Error: 
 Accuracy: 23.8%, Avg loss: 3.319544 

Epoch 99
-------------------------------
loss: 2.549667  [   64/50000]
loss: 2.277612  [ 6464/50000]
loss: 2.191401  [12864/50000]
loss: 2.384742  [19264/50000]
loss: 2.196829  [25664/50000]
loss: 2.407347  [32064/50000]
loss: 2.479132  [38464/50000]
loss: 2.454540  [44864/50000]
Test Error: 
 Accuracy: 23.5%, Avg loss: 3.300346 

Epoch 100
-------------------------------
loss: 2.464565  [   64/50000]
loss: 2.349000  [ 6464/50000]
loss: 2.600848  [12864/50000]
loss: 2.364665  [19264/50000]
loss: 2.393705  [25664/50000]
loss: 2.390971  [32064/50000]
loss: 2.397997  [38464/50000]
loss: 2.451651  [44864/50000]
Test Error: 
 Accuracy: 24.8%, Avg loss: 3.218573 

Done!
Loaded results from results/CIFAR100.csv
Model already exists in results, will remove old results
Saved results to results/CIFAR100.csv
Saved PyTorch Model State to saved_models/FC_FF_NN.pth
Loaded results from results/CIFAR100.csv

 
 
---------------------------- New Experiment ----------------------------
Data: CIFAR100
Model: CNN
Batch size: 64
Learning rate: 0.008
Epochs: 100
Load model: False
Save model: True
Using cuda device
Files already downloaded and verified
Files already downloaded and verified
Epoch 1
-------------------------------
loss: 4.612597  [   64/50000]
loss: 4.596537  [ 6464/50000]
loss: 4.603098  [12864/50000]
loss: 4.599549  [19264/50000]
loss: 4.601531  [25664/50000]
loss: 4.602909  [32064/50000]
loss: 4.592231  [38464/50000]
loss: 4.605827  [44864/50000]
Test Error: 
 Accuracy: 1.1%, Avg loss: 4.605616 

Epoch 2
-------------------------------
loss: 4.595708  [   64/50000]
loss: 4.615558  [ 6464/50000]
loss: 4.609081  [12864/50000]
loss: 4.603330  [19264/50000]
loss: 4.603583  [25664/50000]
loss: 4.608560  [32064/50000]
loss: 4.608419  [38464/50000]
loss: 4.608468  [44864/50000]
Test Error: 
 Accuracy: 1.2%, Avg loss: 4.604291 

Epoch 3
-------------------------------
loss: 4.604634  [   64/50000]
loss: 4.595747  [ 6464/50000]
loss: 4.607249  [12864/50000]
loss: 4.608515  [19264/50000]
loss: 4.604527  [25664/50000]
loss: 4.605904  [32064/50000]
loss: 4.603064  [38464/50000]
loss: 4.601935  [44864/50000]
Test Error: 
 Accuracy: 1.4%, Avg loss: 4.602795 

Epoch 4
-------------------------------
loss: 4.601447  [   64/50000]
loss: 4.608256  [ 6464/50000]
loss: 4.603268  [12864/50000]
loss: 4.607832  [19264/50000]
loss: 4.594905  [25664/50000]
loss: 4.593587  [32064/50000]
loss: 4.608553  [38464/50000]
loss: 4.592231  [44864/50000]
Test Error: 
 Accuracy: 1.2%, Avg loss: 4.600352 

Epoch 5
-------------------------------
loss: 4.601491  [   64/50000]
loss: 4.601158  [ 6464/50000]
loss: 4.606635  [12864/50000]
loss: 4.596869  [19264/50000]
loss: 4.594152  [25664/50000]
loss: 4.600070  [32064/50000]
loss: 4.595445  [38464/50000]
loss: 4.602488  [44864/50000]
Test Error: 
 Accuracy: 1.2%, Avg loss: 4.596058 

Epoch 6
-------------------------------
loss: 4.601855  [   64/50000]
loss: 4.605256  [ 6464/50000]
loss: 4.607304  [12864/50000]
loss: 4.586106  [19264/50000]
loss: 4.583738  [25664/50000]
loss: 4.593632  [32064/50000]
loss: 4.585855  [38464/50000]
loss: 4.573967  [44864/50000]
Test Error: 
 Accuracy: 1.1%, Avg loss: 4.587342 

Epoch 7
-------------------------------
loss: 4.580047  [   64/50000]
loss: 4.586971  [ 6464/50000]
loss: 4.568404  [12864/50000]
loss: 4.597279  [19264/50000]
loss: 4.574955  [25664/50000]
loss: 4.563871  [32064/50000]
loss: 4.571757  [38464/50000]
loss: 4.562666  [44864/50000]
Test Error: 
 Accuracy: 2.2%, Avg loss: 4.555329 

Epoch 8
-------------------------------
loss: 4.541421  [   64/50000]
loss: 4.519853  [ 6464/50000]
loss: 4.525687  [12864/50000]
loss: 4.477876  [19264/50000]
loss: 4.331891  [25664/50000]
loss: 4.382059  [32064/50000]
loss: 4.568454  [38464/50000]
loss: 4.490982  [44864/50000]
Test Error: 
 Accuracy: 3.8%, Avg loss: 4.328074 

Epoch 9
-------------------------------
loss: 4.469625  [   64/50000]
loss: 4.339833  [ 6464/50000]
loss: 4.346261  [12864/50000]
loss: 4.325374  [19264/50000]
loss: 4.210371  [25664/50000]
loss: 4.200879  [32064/50000]
loss: 4.232012  [38464/50000]
loss: 4.261561  [44864/50000]
Test Error: 
 Accuracy: 5.4%, Avg loss: 4.178318 

Epoch 10
-------------------------------
loss: 4.367408  [   64/50000]
loss: 4.255547  [ 6464/50000]
loss: 4.148217  [12864/50000]
loss: 4.292281  [19264/50000]
loss: 4.190259  [25664/50000]
loss: 4.095520  [32064/50000]
loss: 4.007903  [38464/50000]
loss: 3.815350  [44864/50000]
Test Error: 
 Accuracy: 7.4%, Avg loss: 4.108762 

Epoch 11
-------------------------------
loss: 3.907952  [   64/50000]
loss: 4.016782  [ 6464/50000]
loss: 4.046109  [12864/50000]
loss: 4.081185  [19264/50000]
loss: 4.176138  [25664/50000]
loss: 3.827360  [32064/50000]
loss: 3.981522  [38464/50000]
loss: 3.999276  [44864/50000]
Test Error: 
 Accuracy: 7.9%, Avg loss: 4.070411 

Epoch 12
-------------------------------
loss: 4.055792  [   64/50000]
loss: 3.926884  [ 6464/50000]
loss: 3.742266  [12864/50000]
loss: 4.002816  [19264/50000]
loss: 3.962104  [25664/50000]
loss: 3.811388  [32064/50000]
loss: 3.900701  [38464/50000]
loss: 4.074236  [44864/50000]
Test Error: 
 Accuracy: 10.0%, Avg loss: 3.953595 

Epoch 13
-------------------------------
loss: 3.830274  [   64/50000]
loss: 3.830289  [ 6464/50000]
loss: 3.880476  [12864/50000]
loss: 4.053029  [19264/50000]
loss: 3.911966  [25664/50000]
loss: 3.919245  [32064/50000]
loss: 3.928455  [38464/50000]
loss: 3.670086  [44864/50000]
Test Error: 
 Accuracy: 10.8%, Avg loss: 3.908762 

Epoch 14
-------------------------------
loss: 3.965463  [   64/50000]
loss: 3.913862  [ 6464/50000]
loss: 3.746784  [12864/50000]
loss: 3.811160  [19264/50000]
loss: 3.852456  [25664/50000]
loss: 3.862077  [32064/50000]
loss: 3.642983  [38464/50000]
loss: 4.050992  [44864/50000]
Test Error: 
 Accuracy: 11.0%, Avg loss: 3.898697 

Epoch 15
-------------------------------
loss: 3.694955  [   64/50000]
loss: 3.815266  [ 6464/50000]
loss: 3.833059  [12864/50000]
loss: 4.100302  [19264/50000]
loss: 3.562617  [25664/50000]
loss: 4.046903  [32064/50000]
loss: 3.937701  [38464/50000]
loss: 3.810045  [44864/50000]
Test Error: 
 Accuracy: 12.4%, Avg loss: 3.789175 

Epoch 16
-------------------------------
loss: 3.715218  [   64/50000]
loss: 3.532568  [ 6464/50000]
loss: 4.190685  [12864/50000]
loss: 3.884152  [19264/50000]
loss: 3.773630  [25664/50000]
loss: 3.548218  [32064/50000]
loss: 3.935840  [38464/50000]
loss: 3.528539  [44864/50000]
Test Error: 
 Accuracy: 12.1%, Avg loss: 3.815012 

Epoch 17
-------------------------------
loss: 3.934769  [   64/50000]
loss: 3.808945  [ 6464/50000]
loss: 3.410219  [12864/50000]
loss: 3.635654  [19264/50000]
loss: 3.600435  [25664/50000]
loss: 3.610202  [32064/50000]
loss: 3.609242  [38464/50000]
loss: 3.733733  [44864/50000]
Test Error: 
 Accuracy: 14.1%, Avg loss: 3.715077 

Epoch 18
-------------------------------
loss: 3.754241  [   64/50000]
loss: 3.426013  [ 6464/50000]
loss: 3.596817  [12864/50000]
loss: 3.559427  [19264/50000]
loss: 3.621225  [25664/50000]
loss: 3.470762  [32064/50000]
loss: 3.656733  [38464/50000]
loss: 3.563844  [44864/50000]
Test Error: 
 Accuracy: 13.5%, Avg loss: 3.766058 

Epoch 19
-------------------------------
loss: 3.553582  [   64/50000]
loss: 3.446833  [ 6464/50000]
loss: 3.541693  [12864/50000]
loss: 3.525988  [19264/50000]
loss: 3.701923  [25664/50000]
loss: 3.491578  [32064/50000]
loss: 3.471017  [38464/50000]
loss: 3.686744  [44864/50000]
Test Error: 
 Accuracy: 15.4%, Avg loss: 3.620705 

Epoch 20
-------------------------------
loss: 3.748195  [   64/50000]
loss: 3.398152  [ 6464/50000]
loss: 3.740147  [12864/50000]
loss: 3.427845  [19264/50000]
loss: 3.674283  [25664/50000]
loss: 3.653728  [32064/50000]
loss: 3.638741  [38464/50000]
loss: 3.476942  [44864/50000]
Test Error: 
 Accuracy: 14.8%, Avg loss: 3.676646 

Epoch 21
-------------------------------
loss: 3.907578  [   64/50000]
loss: 3.714730  [ 6464/50000]
loss: 3.731068  [12864/50000]
loss: 3.508791  [19264/50000]
loss: 3.749990  [25664/50000]
loss: 3.436443  [32064/50000]
loss: 3.319255  [38464/50000]
loss: 3.308913  [44864/50000]
Test Error: 
 Accuracy: 14.0%, Avg loss: 3.755596 

Epoch 22
-------------------------------
loss: 3.709108  [   64/50000]
loss: 3.196223  [ 6464/50000]
loss: 3.078570  [12864/50000]
loss: 3.163183  [19264/50000]
loss: 3.255455  [25664/50000]
loss: 3.288472  [32064/50000]
loss: 3.483276  [38464/50000]
loss: 3.198499  [44864/50000]
Test Error: 
 Accuracy: 16.4%, Avg loss: 3.542542 

Epoch 23
-------------------------------
loss: 3.360442  [   64/50000]
loss: 3.348879  [ 6464/50000]
loss: 3.543250  [12864/50000]
loss: 3.494521  [19264/50000]
loss: 3.143381  [25664/50000]
loss: 3.474754  [32064/50000]
loss: 3.630563  [38464/50000]
loss: 3.470209  [44864/50000]
Test Error: 
 Accuracy: 18.6%, Avg loss: 3.443984 

Epoch 24
-------------------------------
loss: 3.468040  [   64/50000]
loss: 3.182297  [ 6464/50000]
loss: 3.088253  [12864/50000]
loss: 3.537551  [19264/50000]
loss: 3.537533  [25664/50000]
loss: 3.190679  [32064/50000]
loss: 3.149020  [38464/50000]
loss: 3.205278  [44864/50000]
Test Error: 
 Accuracy: 19.2%, Avg loss: 3.420969 

Epoch 25
-------------------------------
loss: 3.348515  [   64/50000]
loss: 3.214468  [ 6464/50000]
loss: 3.088841  [12864/50000]
loss: 3.590245  [19264/50000]
loss: 3.273202  [25664/50000]
loss: 3.082012  [32064/50000]
loss: 3.046512  [38464/50000]
loss: 3.100498  [44864/50000]
Test Error: 
 Accuracy: 19.3%, Avg loss: 3.401583 

Epoch 26
-------------------------------
loss: 3.464958  [   64/50000]
loss: 2.779844  [ 6464/50000]
loss: 2.954977  [12864/50000]
loss: 3.144395  [19264/50000]
loss: 3.060701  [25664/50000]
loss: 3.181375  [32064/50000]
loss: 3.108816  [38464/50000]
loss: 3.518152  [44864/50000]
Test Error: 
 Accuracy: 20.7%, Avg loss: 3.323130 

Epoch 27
-------------------------------
loss: 3.114527  [   64/50000]
loss: 3.598966  [ 6464/50000]
loss: 3.472721  [12864/50000]
loss: 3.105945  [19264/50000]
loss: 3.185275  [25664/50000]
loss: 3.289490  [32064/50000]
loss: 3.302877  [38464/50000]
loss: 3.088896  [44864/50000]
Test Error: 
 Accuracy: 21.8%, Avg loss: 3.280645 

Epoch 28
-------------------------------
loss: 3.603228  [   64/50000]
loss: 3.391472  [ 6464/50000]
loss: 2.909519  [12864/50000]
loss: 3.498702  [19264/50000]
loss: 3.353473  [25664/50000]
loss: 3.205075  [32064/50000]
loss: 3.198419  [38464/50000]
loss: 3.052004  [44864/50000]
Test Error: 
 Accuracy: 22.9%, Avg loss: 3.242596 

Epoch 29
-------------------------------
loss: 3.137862  [   64/50000]
loss: 3.079974  [ 6464/50000]
loss: 3.238204  [12864/50000]
loss: 2.981928  [19264/50000]
loss: 3.020993  [25664/50000]
loss: 3.191714  [32064/50000]
loss: 3.092895  [38464/50000]
loss: 3.236537  [44864/50000]
Test Error: 
 Accuracy: 22.8%, Avg loss: 3.217643 

Epoch 30
-------------------------------
loss: 2.849703  [   64/50000]
loss: 2.819672  [ 6464/50000]
loss: 2.866179  [12864/50000]
loss: 3.075375  [19264/50000]
loss: 3.262983  [25664/50000]
loss: 3.228991  [32064/50000]
loss: 3.184469  [38464/50000]
loss: 3.232926  [44864/50000]
Test Error: 
 Accuracy: 19.4%, Avg loss: 3.445905 

Epoch 31
-------------------------------
loss: 3.777190  [   64/50000]
loss: 3.132808  [ 6464/50000]
loss: 2.960581  [12864/50000]
loss: 3.355201  [19264/50000]
loss: 3.370900  [25664/50000]
loss: 3.170520  [32064/50000]
loss: 2.921095  [38464/50000]
loss: 3.025346  [44864/50000]
Test Error: 
 Accuracy: 23.1%, Avg loss: 3.216028 

Epoch 32
-------------------------------
loss: 3.359566  [   64/50000]
loss: 2.807456  [ 6464/50000]
loss: 2.874524  [12864/50000]
loss: 2.784475  [19264/50000]
loss: 3.260045  [25664/50000]
loss: 2.933402  [32064/50000]
loss: 3.055187  [38464/50000]
loss: 2.904733  [44864/50000]
Test Error: 
 Accuracy: 24.5%, Avg loss: 3.143204 

Epoch 33
-------------------------------
loss: 2.880396  [   64/50000]
loss: 3.138043  [ 6464/50000]
loss: 2.969560  [12864/50000]
loss: 2.637855  [19264/50000]
loss: 2.883602  [25664/50000]
loss: 2.887192  [32064/50000]
loss: 2.698083  [38464/50000]
loss: 2.672420  [44864/50000]
Test Error: 
 Accuracy: 23.4%, Avg loss: 3.229259 

Epoch 34
-------------------------------
loss: 3.111793  [   64/50000]
loss: 2.744129  [ 6464/50000]
loss: 2.977290  [12864/50000]
loss: 3.008016  [19264/50000]
loss: 2.986459  [25664/50000]
loss: 2.694418  [32064/50000]
loss: 2.442081  [38464/50000]
loss: 2.734054  [44864/50000]
Test Error: 
 Accuracy: 26.9%, Avg loss: 3.036457 

Epoch 35
-------------------------------
loss: 2.701518  [   64/50000]
loss: 3.362042  [ 6464/50000]
loss: 2.933538  [12864/50000]
loss: 2.925802  [19264/50000]
loss: 2.832992  [25664/50000]
loss: 3.115522  [32064/50000]
loss: 2.756137  [38464/50000]
loss: 3.047645  [44864/50000]
Test Error: 
 Accuracy: 26.5%, Avg loss: 3.022334 

Epoch 36
-------------------------------
loss: 2.968067  [   64/50000]
loss: 2.707495  [ 6464/50000]
loss: 2.695922  [12864/50000]
loss: 2.953817  [19264/50000]
loss: 2.304805  [25664/50000]
loss: 2.728874  [32064/50000]
loss: 2.348951  [38464/50000]
loss: 2.813658  [44864/50000]
Test Error: 
 Accuracy: 28.0%, Avg loss: 2.954256 

Epoch 37
-------------------------------
loss: 2.677180  [   64/50000]
loss: 2.992833  [ 6464/50000]
loss: 2.496212  [12864/50000]
loss: 2.779419  [19264/50000]
loss: 3.139116  [25664/50000]
loss: 2.698473  [32064/50000]
loss: 2.459354  [38464/50000]
loss: 2.739896  [44864/50000]
Test Error: 
 Accuracy: 26.6%, Avg loss: 3.028694 

Epoch 38
-------------------------------
loss: 3.055987  [   64/50000]
loss: 2.912703  [ 6464/50000]
loss: 2.550970  [12864/50000]
loss: 3.146907  [19264/50000]
loss: 2.775073  [25664/50000]
loss: 2.890059  [32064/50000]
loss: 2.568221  [38464/50000]
loss: 2.266954  [44864/50000]
Test Error: 
 Accuracy: 21.2%, Avg loss: 3.567928 

Epoch 39
-------------------------------
loss: 3.371787  [   64/50000]
loss: 2.717036  [ 6464/50000]
loss: 2.554127  [12864/50000]
loss: 2.421118  [19264/50000]
loss: 2.854940  [25664/50000]
loss: 2.646016  [32064/50000]
loss: 2.524959  [38464/50000]
loss: 2.428250  [44864/50000]
Test Error: 
 Accuracy: 27.9%, Avg loss: 2.973267 

Epoch 40
-------------------------------
loss: 2.741774  [   64/50000]
loss: 2.543533  [ 6464/50000]
loss: 2.903620  [12864/50000]
loss: 2.508964  [19264/50000]
loss: 2.814732  [25664/50000]
loss: 2.259742  [32064/50000]
loss: 2.747694  [38464/50000]
loss: 2.741468  [44864/50000]
Test Error: 
 Accuracy: 26.3%, Avg loss: 3.116571 

Epoch 41
-------------------------------
loss: 3.056074  [   64/50000]
loss: 2.641851  [ 6464/50000]
loss: 2.676215  [12864/50000]
loss: 2.621148  [19264/50000]
loss: 2.519431  [25664/50000]
loss: 2.341833  [32064/50000]
loss: 2.980532  [38464/50000]
loss: 2.671978  [44864/50000]
Test Error: 
 Accuracy: 30.1%, Avg loss: 2.864062 

Epoch 42
-------------------------------
loss: 2.438089  [   64/50000]
loss: 2.346913  [ 6464/50000]
loss: 2.584391  [12864/50000]
loss: 2.493443  [19264/50000]
loss: 2.577581  [25664/50000]
loss: 2.532733  [32064/50000]
loss: 3.094474  [38464/50000]
loss: 2.414252  [44864/50000]
Test Error: 
 Accuracy: 29.3%, Avg loss: 2.897190 

Epoch 43
-------------------------------
loss: 2.510201  [   64/50000]
loss: 2.476902  [ 6464/50000]
loss: 3.068264  [12864/50000]
loss: 2.928862  [19264/50000]
loss: 2.479424  [25664/50000]
loss: 2.752548  [32064/50000]
loss: 2.924081  [38464/50000]
loss: 2.794743  [44864/50000]
Test Error: 
 Accuracy: 29.2%, Avg loss: 2.946580 

Epoch 44
-------------------------------
loss: 2.566583  [   64/50000]
loss: 2.500986  [ 6464/50000]
loss: 2.921323  [12864/50000]
loss: 2.315641  [19264/50000]
loss: 2.558928  [25664/50000]
loss: 2.728531  [32064/50000]
loss: 2.733939  [38464/50000]
loss: 2.095847  [44864/50000]
Test Error: 
 Accuracy: 29.6%, Avg loss: 2.897547 

Epoch 45
-------------------------------
loss: 2.671703  [   64/50000]
loss: 2.569690  [ 6464/50000]
loss: 2.404058  [12864/50000]
loss: 2.371751  [19264/50000]
loss: 2.222812  [25664/50000]
loss: 2.311249  [32064/50000]
loss: 2.503406  [38464/50000]
loss: 2.753428  [44864/50000]
Test Error: 
 Accuracy: 26.2%, Avg loss: 3.127888 

Epoch 46
-------------------------------
loss: 2.745414  [   64/50000]
loss: 2.181186  [ 6464/50000]
loss: 2.562958  [12864/50000]
loss: 2.255201  [19264/50000]
loss: 2.265185  [25664/50000]
loss: 2.541197  [32064/50000]
loss: 2.396524  [38464/50000]
loss: 2.863764  [44864/50000]
Test Error: 
 Accuracy: 31.4%, Avg loss: 2.777190 

Epoch 47
-------------------------------
loss: 2.493227  [   64/50000]
loss: 2.364798  [ 6464/50000]
loss: 2.518009  [12864/50000]
loss: 2.350027  [19264/50000]
loss: 2.147835  [25664/50000]
loss: 2.372972  [32064/50000]
loss: 2.448427  [38464/50000]
loss: 2.608115  [44864/50000]
Test Error: 
 Accuracy: 31.9%, Avg loss: 2.766583 

Epoch 48
-------------------------------
loss: 2.581014  [   64/50000]
loss: 2.413320  [ 6464/50000]
loss: 2.273221  [12864/50000]
loss: 2.839934  [19264/50000]
loss: 2.556557  [25664/50000]
loss: 2.057941  [32064/50000]
loss: 2.266140  [38464/50000]
loss: 2.302610  [44864/50000]
Test Error: 
 Accuracy: 30.4%, Avg loss: 2.901345 

Epoch 49
-------------------------------
loss: 2.856802  [   64/50000]
loss: 2.006411  [ 6464/50000]
loss: 2.218625  [12864/50000]
loss: 2.250285  [19264/50000]
loss: 1.926097  [25664/50000]
loss: 2.571667  [32064/50000]
loss: 2.717276  [38464/50000]
loss: 2.295271  [44864/50000]
Test Error: 
 Accuracy: 32.3%, Avg loss: 2.769500 

Epoch 50
-------------------------------
loss: 3.008801  [   64/50000]
loss: 2.356107  [ 6464/50000]
loss: 2.543005  [12864/50000]
loss: 2.383858  [19264/50000]
loss: 2.944150  [25664/50000]
loss: 2.142424  [32064/50000]
loss: 2.564444  [38464/50000]
loss: 2.332648  [44864/50000]
Test Error: 
 Accuracy: 31.5%, Avg loss: 2.792555 

Epoch 51
-------------------------------
loss: 2.265776  [   64/50000]
loss: 2.734256  [ 6464/50000]
loss: 2.502045  [12864/50000]
loss: 2.716915  [19264/50000]
loss: 2.327577  [25664/50000]
loss: 2.269559  [32064/50000]
loss: 2.562442  [38464/50000]
loss: 2.455384  [44864/50000]
Test Error: 
 Accuracy: 32.1%, Avg loss: 2.784472 

Epoch 52
-------------------------------
loss: 2.517983  [   64/50000]
loss: 2.277541  [ 6464/50000]
loss: 2.786020  [12864/50000]
loss: 2.278781  [19264/50000]
loss: 2.208644  [25664/50000]
loss: 2.664835  [32064/50000]
loss: 2.598998  [38464/50000]
loss: 2.135933  [44864/50000]
Test Error: 
 Accuracy: 34.0%, Avg loss: 2.686161 

Epoch 53
-------------------------------
loss: 1.942818  [   64/50000]
loss: 2.488303  [ 6464/50000]
loss: 2.411587  [12864/50000]
loss: 2.389240  [19264/50000]
loss: 2.379771  [25664/50000]
loss: 2.203901  [32064/50000]
loss: 2.733697  [38464/50000]
loss: 2.066108  [44864/50000]
Test Error: 
 Accuracy: 30.3%, Avg loss: 2.937727 

Epoch 54
-------------------------------
loss: 2.665548  [   64/50000]
loss: 2.225951  [ 6464/50000]
loss: 2.380832  [12864/50000]
loss: 2.476852  [19264/50000]
loss: 2.385510  [25664/50000]
loss: 2.379638  [32064/50000]
loss: 2.219285  [38464/50000]
loss: 2.173628  [44864/50000]
Test Error: 
 Accuracy: 31.7%, Avg loss: 2.797751 

Epoch 55
-------------------------------
loss: 2.786916  [   64/50000]
loss: 2.417698  [ 6464/50000]
loss: 2.213738  [12864/50000]
loss: 2.164381  [19264/50000]
loss: 2.460042  [25664/50000]
loss: 2.423889  [32064/50000]
loss: 2.603712  [38464/50000]
loss: 2.468044  [44864/50000]
Test Error: 
 Accuracy: 30.3%, Avg loss: 2.943825 

Epoch 56
-------------------------------
loss: 2.868889  [   64/50000]
loss: 2.242476  [ 6464/50000]
loss: 2.477184  [12864/50000]
loss: 2.497292  [19264/50000]
loss: 2.670341  [25664/50000]
loss: 2.210498  [32064/50000]
loss: 2.084571  [38464/50000]
loss: 2.318281  [44864/50000]
Test Error: 
 Accuracy: 34.1%, Avg loss: 2.679307 

Epoch 57
-------------------------------
loss: 2.170176  [   64/50000]
loss: 2.082787  [ 6464/50000]
loss: 2.086562  [12864/50000]
loss: 2.389062  [19264/50000]
loss: 2.508433  [25664/50000]
loss: 2.456158  [32064/50000]
loss: 2.147457  [38464/50000]
loss: 2.059459  [44864/50000]
Test Error: 
 Accuracy: 31.6%, Avg loss: 2.809314 

Epoch 58
-------------------------------
loss: 2.451405  [   64/50000]
loss: 2.511200  [ 6464/50000]
loss: 2.418473  [12864/50000]
loss: 2.173676  [19264/50000]
loss: 2.437986  [25664/50000]
loss: 1.984640  [32064/50000]
loss: 2.314085  [38464/50000]
loss: 2.018488  [44864/50000]
Test Error: 
 Accuracy: 31.6%, Avg loss: 2.852437 

Epoch 59
-------------------------------
loss: 2.390780  [   64/50000]
loss: 2.123427  [ 6464/50000]
loss: 2.401484  [12864/50000]
loss: 2.317446  [19264/50000]
loss: 2.327676  [25664/50000]
loss: 2.145429  [32064/50000]
loss: 2.530399  [38464/50000]
loss: 2.483454  [44864/50000]
Test Error: 
 Accuracy: 32.2%, Avg loss: 2.824841 

Epoch 60
-------------------------------
loss: 2.417254  [   64/50000]
loss: 1.873064  [ 6464/50000]
loss: 2.453806  [12864/50000]
loss: 2.083101  [19264/50000]
loss: 2.165722  [25664/50000]
loss: 2.251925  [32064/50000]
loss: 1.693096  [38464/50000]
loss: 2.233548  [44864/50000]
Test Error: 
 Accuracy: 33.4%, Avg loss: 2.747490 

Epoch 61
-------------------------------
loss: 2.236882  [   64/50000]
loss: 2.781913  [ 6464/50000]
loss: 2.328702  [12864/50000]
loss: 2.388973  [19264/50000]
loss: 2.552558  [25664/50000]
loss: 2.479485  [32064/50000]
loss: 2.338465  [38464/50000]
loss: 2.418627  [44864/50000]
Test Error: 
 Accuracy: 34.5%, Avg loss: 2.695026 

Epoch 62
-------------------------------
loss: 2.637978  [   64/50000]
loss: 2.063431  [ 6464/50000]
loss: 1.838258  [12864/50000]
loss: 2.145691  [19264/50000]
loss: 2.341637  [25664/50000]
loss: 2.009360  [32064/50000]
loss: 2.337051  [38464/50000]
loss: 2.037383  [44864/50000]
Test Error: 
 Accuracy: 34.3%, Avg loss: 2.677394 

Epoch 63
-------------------------------
loss: 1.880927  [   64/50000]
loss: 1.969958  [ 6464/50000]
loss: 2.214116  [12864/50000]
loss: 2.172219  [19264/50000]
loss: 2.260348  [25664/50000]
loss: 1.930468  [32064/50000]
loss: 1.864378  [38464/50000]
loss: 2.029487  [44864/50000]
Test Error: 
 Accuracy: 33.0%, Avg loss: 2.809891 

Epoch 64
-------------------------------
loss: 2.456273  [   64/50000]
loss: 2.274225  [ 6464/50000]
loss: 2.511957  [12864/50000]
loss: 2.560507  [19264/50000]
loss: 1.848956  [25664/50000]
loss: 1.943953  [32064/50000]
loss: 1.522051  [38464/50000]
loss: 2.244700  [44864/50000]
Test Error: 
 Accuracy: 35.1%, Avg loss: 2.664048 

Epoch 65
-------------------------------
loss: 2.560503  [   64/50000]
loss: 2.816900  [ 6464/50000]
loss: 1.943023  [12864/50000]
loss: 2.070347  [19264/50000]
loss: 2.041078  [25664/50000]
loss: 1.993779  [32064/50000]
loss: 2.076904  [38464/50000]
loss: 1.995030  [44864/50000]
Test Error: 
 Accuracy: 34.0%, Avg loss: 2.706274 

Epoch 66
-------------------------------
loss: 2.175601  [   64/50000]
loss: 2.120349  [ 6464/50000]
loss: 1.848865  [12864/50000]
loss: 2.177149  [19264/50000]
loss: 2.033184  [25664/50000]
loss: 1.707930  [32064/50000]
loss: 2.562313  [38464/50000]
loss: 1.740395  [44864/50000]
Test Error: 
 Accuracy: 34.3%, Avg loss: 2.699555 

Epoch 67
-------------------------------
loss: 2.229028  [   64/50000]
loss: 2.237107  [ 6464/50000]
loss: 1.956699  [12864/50000]
loss: 1.897954  [19264/50000]
loss: 2.145297  [25664/50000]
loss: 1.846143  [32064/50000]
loss: 2.240554  [38464/50000]
loss: 2.130185  [44864/50000]
Test Error: 
 Accuracy: 35.4%, Avg loss: 2.679003 

Epoch 68
-------------------------------
loss: 2.194655  [   64/50000]
loss: 1.747102  [ 6464/50000]
loss: 2.207514  [12864/50000]
loss: 1.935454  [19264/50000]
loss: 2.226473  [25664/50000]
loss: 1.948216  [32064/50000]
loss: 2.006381  [38464/50000]
loss: 1.843747  [44864/50000]
Test Error: 
 Accuracy: 35.5%, Avg loss: 2.663967 

Epoch 69
-------------------------------
loss: 2.376254  [   64/50000]
loss: 2.410916  [ 6464/50000]
loss: 2.024829  [12864/50000]
loss: 1.907335  [19264/50000]
loss: 1.932700  [25664/50000]
loss: 1.973933  [32064/50000]
loss: 2.321913  [38464/50000]
loss: 2.364353  [44864/50000]
Test Error: 
 Accuracy: 34.2%, Avg loss: 2.744759 

Epoch 70
-------------------------------
loss: 2.350698  [   64/50000]
loss: 2.449313  [ 6464/50000]
loss: 1.919305  [12864/50000]
loss: 2.405604  [19264/50000]
loss: 2.556003  [25664/50000]
loss: 2.098269  [32064/50000]
loss: 1.834701  [38464/50000]
loss: 2.315933  [44864/50000]
Test Error: 
 Accuracy: 35.9%, Avg loss: 2.645431 

Epoch 71
-------------------------------
loss: 1.922482  [   64/50000]
loss: 2.273245  [ 6464/50000]
loss: 1.946734  [12864/50000]
loss: 2.026696  [19264/50000]
loss: 2.481167  [25664/50000]
loss: 1.971489  [32064/50000]
loss: 2.011105  [38464/50000]
loss: 2.118977  [44864/50000]
Test Error: 
 Accuracy: 36.4%, Avg loss: 2.647652 

Epoch 72
-------------------------------
loss: 1.770146  [   64/50000]
loss: 1.804712  [ 6464/50000]
loss: 1.829628  [12864/50000]
loss: 1.681207  [19264/50000]
loss: 2.081174  [25664/50000]
loss: 2.209838  [32064/50000]
loss: 2.161606  [38464/50000]
loss: 2.153600  [44864/50000]
Test Error: 
 Accuracy: 31.7%, Avg loss: 2.950422 

Epoch 73
-------------------------------
loss: 2.126563  [   64/50000]
loss: 2.342391  [ 6464/50000]
loss: 1.890382  [12864/50000]
loss: 1.931495  [19264/50000]
loss: 1.581409  [25664/50000]
loss: 2.136630  [32064/50000]
loss: 1.743049  [38464/50000]
loss: 2.144611  [44864/50000]
Test Error: 
 Accuracy: 34.4%, Avg loss: 2.787509 

Epoch 74
-------------------------------
loss: 2.433853  [   64/50000]
loss: 1.762037  [ 6464/50000]
loss: 2.127273  [12864/50000]
loss: 1.839703  [19264/50000]
loss: 1.835106  [25664/50000]
loss: 2.087037  [32064/50000]
loss: 1.782077  [38464/50000]
loss: 1.990914  [44864/50000]
Test Error: 
 Accuracy: 33.5%, Avg loss: 2.846628 

Epoch 75
-------------------------------
loss: 2.094592  [   64/50000]
loss: 1.903816  [ 6464/50000]
loss: 1.869520  [12864/50000]
loss: 2.008723  [19264/50000]
loss: 1.672256  [25664/50000]
loss: 1.929447  [32064/50000]
loss: 1.941254  [38464/50000]
loss: 1.853299  [44864/50000]
Test Error: 
 Accuracy: 34.6%, Avg loss: 2.744431 

Epoch 76
-------------------------------
loss: 2.132506  [   64/50000]
loss: 1.888150  [ 6464/50000]
loss: 1.921709  [12864/50000]
loss: 1.807212  [19264/50000]
loss: 1.740259  [25664/50000]
loss: 1.985238  [32064/50000]
loss: 2.319776  [38464/50000]
loss: 2.032543  [44864/50000]
Test Error: 
 Accuracy: 36.0%, Avg loss: 2.651222 

Epoch 77
-------------------------------
loss: 1.487729  [   64/50000]
loss: 1.663981  [ 6464/50000]
loss: 1.679010  [12864/50000]
loss: 1.881414  [19264/50000]
loss: 1.829936  [25664/50000]
loss: 2.253689  [32064/50000]
loss: 1.773148  [38464/50000]
loss: 1.383305  [44864/50000]
Test Error: 
 Accuracy: 34.3%, Avg loss: 2.828028 

Epoch 78
-------------------------------
loss: 1.908054  [   64/50000]
loss: 1.977369  [ 6464/50000]
loss: 1.732582  [12864/50000]
loss: 1.563960  [19264/50000]
loss: 1.849479  [25664/50000]
loss: 2.005259  [32064/50000]
loss: 1.926644  [38464/50000]
loss: 1.648241  [44864/50000]
Test Error: 
 Accuracy: 34.6%, Avg loss: 2.756087 

Epoch 79
-------------------------------
loss: 2.125706  [   64/50000]
loss: 2.166003  [ 6464/50000]
loss: 1.932689  [12864/50000]
loss: 1.973639  [19264/50000]
loss: 1.853039  [25664/50000]
loss: 2.172449  [32064/50000]
loss: 1.861122  [38464/50000]
loss: 2.124874  [44864/50000]
Test Error: 
 Accuracy: 34.5%, Avg loss: 2.733392 

Epoch 80
-------------------------------
loss: 2.182699  [   64/50000]
loss: 1.708122  [ 6464/50000]
loss: 1.593151  [12864/50000]
loss: 1.612676  [19264/50000]
loss: 1.826680  [25664/50000]
loss: 1.662475  [32064/50000]
loss: 2.284152  [38464/50000]
loss: 1.842470  [44864/50000]
Test Error: 
 Accuracy: 35.1%, Avg loss: 2.785536 

Epoch 81
-------------------------------
loss: 2.445957  [   64/50000]
loss: 1.750720  [ 6464/50000]
loss: 1.689646  [12864/50000]
loss: 1.830177  [19264/50000]
loss: 1.830123  [25664/50000]
loss: 1.866021  [32064/50000]
loss: 1.907844  [38464/50000]
loss: 2.194824  [44864/50000]
Test Error: 
 Accuracy: 34.6%, Avg loss: 2.814945 

Epoch 82
-------------------------------
loss: 2.048409  [   64/50000]
loss: 1.820531  [ 6464/50000]
loss: 1.444082  [12864/50000]
loss: 2.060326  [19264/50000]
loss: 1.812769  [25664/50000]
loss: 2.167374  [32064/50000]
loss: 1.560592  [38464/50000]
loss: 1.910758  [44864/50000]
Test Error: 
 Accuracy: 35.2%, Avg loss: 2.805251 

Epoch 83
-------------------------------
loss: 1.725833  [   64/50000]
loss: 1.496724  [ 6464/50000]
loss: 1.514473  [12864/50000]
loss: 2.127200  [19264/50000]
loss: 1.424402  [25664/50000]
loss: 1.596191  [32064/50000]
loss: 1.790534  [38464/50000]
loss: 1.750317  [44864/50000]
Test Error: 
 Accuracy: 37.4%, Avg loss: 2.680530 

Epoch 84
-------------------------------
loss: 1.719731  [   64/50000]
loss: 1.733086  [ 6464/50000]
loss: 1.871520  [12864/50000]
loss: 1.697430  [19264/50000]
loss: 1.659562  [25664/50000]
loss: 1.823680  [32064/50000]
loss: 1.691157  [38464/50000]
loss: 1.422540  [44864/50000]
Test Error: 
 Accuracy: 36.8%, Avg loss: 2.692121 

Epoch 85
-------------------------------
loss: 1.786452  [   64/50000]
loss: 1.509576  [ 6464/50000]
loss: 1.665463  [12864/50000]
loss: 1.567462  [19264/50000]
loss: 1.814257  [25664/50000]
loss: 1.800041  [32064/50000]
loss: 1.817581  [38464/50000]
loss: 1.861414  [44864/50000]
Test Error: 
 Accuracy: 32.9%, Avg loss: 3.033048 

Epoch 86
-------------------------------
loss: 2.376101  [   64/50000]
loss: 1.699466  [ 6464/50000]
loss: 1.586171  [12864/50000]
loss: 1.906599  [19264/50000]
loss: 1.600862  [25664/50000]
loss: 1.814199  [32064/50000]
loss: 1.772204  [38464/50000]
loss: 1.830674  [44864/50000]
Test Error: 
 Accuracy: 35.9%, Avg loss: 2.773777 

Epoch 87
-------------------------------
loss: 1.559496  [   64/50000]
loss: 1.585042  [ 6464/50000]
loss: 1.498855  [12864/50000]
loss: 1.391437  [19264/50000]
loss: 1.729250  [25664/50000]
loss: 1.568192  [32064/50000]
loss: 1.764619  [38464/50000]
loss: 1.701162  [44864/50000]
Test Error: 
 Accuracy: 36.3%, Avg loss: 2.746658 

Epoch 88
-------------------------------
loss: 1.585873  [   64/50000]
loss: 1.671564  [ 6464/50000]
loss: 1.801473  [12864/50000]
loss: 1.730831  [19264/50000]
loss: 1.508883  [25664/50000]
loss: 1.608054  [32064/50000]
loss: 1.630708  [38464/50000]
loss: 1.908830  [44864/50000]
Test Error: 
 Accuracy: 36.7%, Avg loss: 2.793874 

Epoch 89
-------------------------------
loss: 1.819039  [   64/50000]
loss: 1.856642  [ 6464/50000]
loss: 1.486641  [12864/50000]
loss: 1.997200  [19264/50000]
loss: 1.722978  [25664/50000]
loss: 1.557003  [32064/50000]
loss: 1.839647  [38464/50000]
loss: 1.388853  [44864/50000]
Test Error: 
 Accuracy: 33.1%, Avg loss: 3.023235 

Epoch 90
-------------------------------
loss: 2.071711  [   64/50000]
loss: 1.780439  [ 6464/50000]
loss: 1.804081  [12864/50000]
loss: 1.711422  [19264/50000]
loss: 1.619434  [25664/50000]
loss: 1.560539  [32064/50000]
loss: 1.433070  [38464/50000]
loss: 1.778703  [44864/50000]
Test Error: 
 Accuracy: 35.4%, Avg loss: 2.863611 

Epoch 91
-------------------------------
loss: 2.013921  [   64/50000]
loss: 1.784210  [ 6464/50000]
loss: 1.661849  [12864/50000]
loss: 1.663742  [19264/50000]
loss: 1.313416  [25664/50000]
loss: 1.533199  [32064/50000]
loss: 1.956223  [38464/50000]
loss: 1.636364  [44864/50000]
Test Error: 
 Accuracy: 35.7%, Avg loss: 2.782874 

Epoch 92
-------------------------------
loss: 1.629815  [   64/50000]
loss: 1.929121  [ 6464/50000]
loss: 1.691398  [12864/50000]
loss: 1.673337  [19264/50000]
loss: 1.633266  [25664/50000]
loss: 1.476995  [32064/50000]
loss: 1.642235  [38464/50000]
loss: 1.550384  [44864/50000]
Test Error: 
 Accuracy: 32.8%, Avg loss: 3.046754 

Epoch 93
-------------------------------
loss: 1.720779  [   64/50000]
loss: 1.488510  [ 6464/50000]
loss: 1.368070  [12864/50000]
loss: 1.527547  [19264/50000]
loss: 1.595596  [25664/50000]
loss: 1.757748  [32064/50000]
loss: 1.456820  [38464/50000]
loss: 1.680192  [44864/50000]
Test Error: 
 Accuracy: 35.8%, Avg loss: 2.822968 

Epoch 94
-------------------------------
loss: 1.648182  [   64/50000]
loss: 1.539181  [ 6464/50000]
loss: 1.536738  [12864/50000]
loss: 1.888465  [19264/50000]
loss: 1.833251  [25664/50000]
loss: 1.689209  [32064/50000]
loss: 1.786963  [38464/50000]
loss: 1.935920  [44864/50000]
Test Error: 
 Accuracy: 36.3%, Avg loss: 2.773988 

Epoch 95
-------------------------------
loss: 1.673316  [   64/50000]
loss: 1.139525  [ 6464/50000]
loss: 1.449264  [12864/50000]
loss: 1.604973  [19264/50000]
loss: 1.315549  [25664/50000]
loss: 1.527869  [32064/50000]
loss: 1.681670  [38464/50000]
loss: 1.427039  [44864/50000]
Test Error: 
 Accuracy: 33.8%, Avg loss: 3.067004 

Epoch 96
-------------------------------
loss: 2.152305  [   64/50000]
loss: 1.384213  [ 6464/50000]
loss: 1.574848  [12864/50000]
loss: 1.411437  [19264/50000]
loss: 1.752667  [25664/50000]
loss: 1.825202  [32064/50000]
loss: 1.425314  [38464/50000]
loss: 1.877592  [44864/50000]
Test Error: 
 Accuracy: 35.0%, Avg loss: 2.952350 

Epoch 97
-------------------------------
loss: 1.331572  [   64/50000]
loss: 1.619237  [ 6464/50000]
loss: 1.388408  [12864/50000]
loss: 1.826238  [19264/50000]
loss: 1.499456  [25664/50000]
loss: 1.694270  [32064/50000]
loss: 1.526240  [38464/50000]
loss: 1.853161  [44864/50000]
Test Error: 
 Accuracy: 37.9%, Avg loss: 2.735860 

Epoch 98
-------------------------------
loss: 1.595705  [   64/50000]
loss: 1.903439  [ 6464/50000]
loss: 1.538172  [12864/50000]
loss: 1.448239  [19264/50000]
loss: 1.663756  [25664/50000]
loss: 1.405840  [32064/50000]
loss: 1.846841  [38464/50000]
loss: 1.812320  [44864/50000]
Test Error: 
 Accuracy: 33.4%, Avg loss: 3.092239 

Epoch 99
-------------------------------
loss: 2.118373  [   64/50000]
loss: 1.058021  [ 6464/50000]
loss: 1.370977  [12864/50000]
loss: 1.450639  [19264/50000]
loss: 1.873394  [25664/50000]
loss: 1.917007  [32064/50000]
loss: 1.380928  [38464/50000]
loss: 1.775504  [44864/50000]
Test Error: 
 Accuracy: 37.0%, Avg loss: 2.842211 

Epoch 100
-------------------------------
loss: 1.564360  [   64/50000]
loss: 1.389317  [ 6464/50000]
loss: 1.307732  [12864/50000]
loss: 1.571668  [19264/50000]
loss: 1.351947  [25664/50000]
loss: 1.431712  [32064/50000]
loss: 1.358182  [38464/50000]
loss: 1.475943  [44864/50000]
Test Error: 
 Accuracy: 33.7%, Avg loss: 3.089538 

Done!
Loaded results from results/CIFAR100.csv
Model already exists in results, will remove old results
Saved results to results/CIFAR100.csv
Saved PyTorch Model State to saved_models/CNN.pth
Loaded results from results/CIFAR100.csv

 
 
---------------------------- New Experiment ----------------------------
Data: CIFAR100
Model: RN50_clip_FC_FF_NN
Batch size: 64
Learning rate: 0.01
Epochs: 100
Load model: False
Save model: True
Using cuda device
DatasetEncoder cuda
Files already downloaded and verified
Files already downloaded and verified
encoding CIFAR100 with RN50_clip:   0%|          | 0/50000 [00:00<?, ?it/s]Unsupported operator aten::avg_pool2d encountered 8 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
Unsupported operator aten::mean encountered 1 time(s)
Unsupported operator aten::add encountered 1 time(s)
Unsupported operator aten::div encountered 2 time(s)
Unsupported operator aten::mul encountered 4 time(s)
Unsupported operator aten::softmax encountered 1 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.token_embedding
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.v_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.ln_1
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.k_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.ln_1
Module never called: model.feature_extractor.feature_extractor.visual.layer3.5.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer2.3.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.ln_final
Module never called: model.feature_extractor.feature_extractor.visual.layer1.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer1.1.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer3.3.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer3.4.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.ln_1
Module never called: model.classifiers
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer3.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.attn
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.q_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer2.1.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8
Module never called: model.feature_extractor.feature_extractor.visual.layer4.1.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer3.1.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer1.0.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer2.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.visual.layer4.2.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.attn
cuda
loaded from ./data/EncodedDatasets//CIFAR100_train_RN50_clip_pretrained_1_ntasks_1.hdf5

encoding CIFAR100 with RN50_clip:   0%|          | 0/10000 [00:00<?, ?it/s][Aencoding CIFAR100 with RN50_clip:   0%|          | 0/50000 [00:00<?, ?it/s]
Unsupported operator aten::avg_pool2d encountered 8 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
Unsupported operator aten::mean encountered 1 time(s)
Unsupported operator aten::add encountered 1 time(s)
Unsupported operator aten::div encountered 2 time(s)
Unsupported operator aten::mul encountered 4 time(s)
Unsupported operator aten::softmax encountered 1 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.token_embedding
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.v_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.ln_1
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.k_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.ln_1
Module never called: model.feature_extractor.feature_extractor.visual.layer3.5.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer2.3.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.ln_final
Module never called: model.feature_extractor.feature_extractor.visual.layer1.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer1.1.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer3.3.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer3.4.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.ln_1
Module never called: model.classifiers
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.5.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer3.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.attn
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.q_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.9.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.10.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer2.1.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8
Module never called: model.feature_extractor.feature_extractor.visual.layer4.1.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.layer3.1.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.ln_2
Module never called: model.feature_extractor.feature_extractor.visual.layer1.0.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.3.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.mlp.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.0.mlp.c_fc
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.6.attn
Module never called: model.feature_extractor.feature_extractor.visual.layer2.2.avgpool
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.8.mlp
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.attn.out_proj
Module never called: model.feature_extractor.feature_extractor.visual.layer4.2.avgpool
Module never called: model.feature_extractor.feature_extractor.visual.attnpool.c_proj
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.ln_2
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.4.ln_1
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.1.mlp.gelu
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.11.attn
Module never called: model.feature_extractor.feature_extractor.transformer.resblocks.7.attn
encoding CIFAR100 with RN50_clip:   0%|          | 0/10000 [00:00<?, ?it/s]
cuda
loaded from ./data/EncodedDatasets//CIFAR100_test_RN50_clip_pretrained_1_ntasks_1.hdf5
Epoch 1
-------------------------------
loss: 4.615782  [   64/50000]
loss: 4.614288  [ 6464/50000]
loss: 4.603615  [12864/50000]
loss: 4.601876  [19264/50000]
loss: 4.607927  [25664/50000]
loss: 4.615295  [32064/50000]
loss: 4.612381  [38464/50000]
loss: 4.601061  [44864/50000]
Test Error: 
 Accuracy: 1.0%, Avg loss: 4.606327 

Epoch 2
-------------------------------
loss: 4.612762  [   64/50000]
loss: 4.607721  [ 6464/50000]
loss: 4.609080  [12864/50000]
loss: 4.608896  [19264/50000]
loss: 4.601928  [25664/50000]
loss: 4.607090  [32064/50000]
loss: 4.602925  [38464/50000]
loss: 4.603883  [44864/50000]
Test Error: 
 Accuracy: 1.0%, Avg loss: 4.606032 

Epoch 3
-------------------------------
loss: 4.605568  [   64/50000]
loss: 4.611126  [ 6464/50000]
loss: 4.600028  [12864/50000]
loss: 4.596848  [19264/50000]
loss: 4.606385  [25664/50000]
loss: 4.603744  [32064/50000]
loss: 4.598484  [38464/50000]
loss: 4.601180  [44864/50000]
Test Error: 
 Accuracy: 1.1%, Avg loss: 4.605817 

Epoch 4
-------------------------------
loss: 4.605604  [   64/50000]
loss: 4.597818  [ 6464/50000]
loss: 4.608998  [12864/50000]
loss: 4.604867  [19264/50000]
loss: 4.608361  [25664/50000]
loss: 4.610663  [32064/50000]
loss: 4.609071  [38464/50000]
loss: 4.613704  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.605595 

Epoch 5
-------------------------------
loss: 4.604732  [   64/50000]
loss: 4.611964  [ 6464/50000]
loss: 4.600457  [12864/50000]
loss: 4.606972  [19264/50000]
loss: 4.606294  [25664/50000]
loss: 4.601982  [32064/50000]
loss: 4.607269  [38464/50000]
loss: 4.607157  [44864/50000]
Test Error: 
 Accuracy: 1.4%, Avg loss: 4.605479 

Epoch 6
-------------------------------
loss: 4.609318  [   64/50000]
loss: 4.604732  [ 6464/50000]
loss: 4.605611  [12864/50000]
loss: 4.603216  [19264/50000]
loss: 4.604502  [25664/50000]
loss: 4.604187  [32064/50000]
loss: 4.601260  [38464/50000]
loss: 4.602674  [44864/50000]
Test Error: 
 Accuracy: 1.4%, Avg loss: 4.605319 

Epoch 7
-------------------------------
loss: 4.613655  [   64/50000]
loss: 4.606425  [ 6464/50000]
loss: 4.606407  [12864/50000]
loss: 4.608809  [19264/50000]
loss: 4.603734  [25664/50000]
loss: 4.604908  [32064/50000]
loss: 4.601855  [38464/50000]
loss: 4.607479  [44864/50000]
Test Error: 
 Accuracy: 1.4%, Avg loss: 4.605157 

Epoch 8
-------------------------------
loss: 4.604743  [   64/50000]
loss: 4.605081  [ 6464/50000]
loss: 4.604608  [12864/50000]
loss: 4.604957  [19264/50000]
loss: 4.602417  [25664/50000]
loss: 4.602883  [32064/50000]
loss: 4.608829  [38464/50000]
loss: 4.604359  [44864/50000]
Test Error: 
 Accuracy: 1.2%, Avg loss: 4.605112 

Epoch 9
-------------------------------
loss: 4.605077  [   64/50000]
loss: 4.599703  [ 6464/50000]
loss: 4.605095  [12864/50000]
loss: 4.606105  [19264/50000]
loss: 4.602653  [25664/50000]
loss: 4.605946  [32064/50000]
loss: 4.603773  [38464/50000]
loss: 4.606630  [44864/50000]
Test Error: 
 Accuracy: 1.2%, Avg loss: 4.604990 

Epoch 10
-------------------------------
loss: 4.601535  [   64/50000]
loss: 4.600678  [ 6464/50000]
loss: 4.604644  [12864/50000]
loss: 4.611372  [19264/50000]
loss: 4.605596  [25664/50000]
loss: 4.603276  [32064/50000]
loss: 4.603226  [38464/50000]
loss: 4.601450  [44864/50000]
Test Error: 
 Accuracy: 1.5%, Avg loss: 4.604920 

Epoch 11
-------------------------------
loss: 4.601816  [   64/50000]
loss: 4.601354  [ 6464/50000]
loss: 4.603671  [12864/50000]
loss: 4.602750  [19264/50000]
loss: 4.603653  [25664/50000]
loss: 4.603009  [32064/50000]
loss: 4.605623  [38464/50000]
loss: 4.604710  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.604820 

Epoch 12
-------------------------------
loss: 4.602945  [   64/50000]
loss: 4.603291  [ 6464/50000]
loss: 4.602280  [12864/50000]
loss: 4.602993  [19264/50000]
loss: 4.607611  [25664/50000]
loss: 4.600505  [32064/50000]
loss: 4.602881  [38464/50000]
loss: 4.603203  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.604757 

Epoch 13
-------------------------------
loss: 4.604524  [   64/50000]
loss: 4.605164  [ 6464/50000]
loss: 4.606136  [12864/50000]
loss: 4.605597  [19264/50000]
loss: 4.602067  [25664/50000]
loss: 4.607065  [32064/50000]
loss: 4.605491  [38464/50000]
loss: 4.605275  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.604635 

Epoch 14
-------------------------------
loss: 4.601512  [   64/50000]
loss: 4.603713  [ 6464/50000]
loss: 4.603727  [12864/50000]
loss: 4.603265  [19264/50000]
loss: 4.605994  [25664/50000]
loss: 4.605303  [32064/50000]
loss: 4.605274  [38464/50000]
loss: 4.608938  [44864/50000]
Test Error: 
 Accuracy: 1.5%, Avg loss: 4.604559 

Epoch 15
-------------------------------
loss: 4.603772  [   64/50000]
loss: 4.605790  [ 6464/50000]
loss: 4.607949  [12864/50000]
loss: 4.603653  [19264/50000]
loss: 4.606839  [25664/50000]
loss: 4.605425  [32064/50000]
loss: 4.604237  [38464/50000]
loss: 4.604925  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.604466 

Epoch 16
-------------------------------
loss: 4.604645  [   64/50000]
loss: 4.602552  [ 6464/50000]
loss: 4.603169  [12864/50000]
loss: 4.606684  [19264/50000]
loss: 4.604188  [25664/50000]
loss: 4.600934  [32064/50000]
loss: 4.603366  [38464/50000]
loss: 4.603620  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.604329 

Epoch 17
-------------------------------
loss: 4.603090  [   64/50000]
loss: 4.606879  [ 6464/50000]
loss: 4.603175  [12864/50000]
loss: 4.604759  [19264/50000]
loss: 4.603014  [25664/50000]
loss: 4.604973  [32064/50000]
loss: 4.605043  [38464/50000]
loss: 4.604345  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.604249 

Epoch 18
-------------------------------
loss: 4.601283  [   64/50000]
loss: 4.603102  [ 6464/50000]
loss: 4.606745  [12864/50000]
loss: 4.604765  [19264/50000]
loss: 4.605403  [25664/50000]
loss: 4.600902  [32064/50000]
loss: 4.604365  [38464/50000]
loss: 4.605514  [44864/50000]
Test Error: 
 Accuracy: 1.7%, Avg loss: 4.604147 

Epoch 19
-------------------------------
loss: 4.603207  [   64/50000]
loss: 4.602285  [ 6464/50000]
loss: 4.604198  [12864/50000]
loss: 4.602396  [19264/50000]
loss: 4.607047  [25664/50000]
loss: 4.603861  [32064/50000]
loss: 4.602287  [38464/50000]
loss: 4.603084  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.604026 

Epoch 20
-------------------------------
loss: 4.604110  [   64/50000]
loss: 4.603291  [ 6464/50000]
loss: 4.604024  [12864/50000]
loss: 4.606926  [19264/50000]
loss: 4.603074  [25664/50000]
loss: 4.604351  [32064/50000]
loss: 4.603775  [38464/50000]
loss: 4.605120  [44864/50000]
Test Error: 
 Accuracy: 1.6%, Avg loss: 4.603884 

Epoch 21
-------------------------------
loss: 4.602999  [   64/50000]
loss: 4.603801  [ 6464/50000]
loss: 4.603328  [12864/50000]
loss: 4.602177  [19264/50000]
loss: 4.603141  [25664/50000]
loss: 4.603721  [32064/50000]
loss: 4.603673  [38464/50000]
loss: 4.605577  [44864/50000]
Test Error: 
 Accuracy: 1.8%, Avg loss: 4.603754 

Epoch 22
-------------------------------
loss: 4.603636  [   64/50000]
loss: 4.602546  [ 6464/50000]
loss: 4.601864  [12864/50000]
loss: 4.606411  [19264/50000]
loss: 4.603150  [25664/50000]
loss: 4.603653  [32064/50000]
loss: 4.603334  [38464/50000]
loss: 4.603502  [44864/50000]
Test Error: 
 Accuracy: 2.5%, Avg loss: 4.603589 

Epoch 23
-------------------------------
loss: 4.603564  [   64/50000]
loss: 4.604192  [ 6464/50000]
loss: 4.603093  [12864/50000]
loss: 4.602731  [19264/50000]
loss: 4.603061  [25664/50000]
loss: 4.605068  [32064/50000]
loss: 4.603976  [38464/50000]
loss: 4.602815  [44864/50000]
Test Error: 
 Accuracy: 2.8%, Avg loss: 4.603394 

Epoch 24
-------------------------------
loss: 4.602523  [   64/50000]
loss: 4.603081  [ 6464/50000]
loss: 4.604931  [12864/50000]
loss: 4.603010  [19264/50000]
loss: 4.603780  [25664/50000]
loss: 4.602557  [32064/50000]
loss: 4.604005  [38464/50000]
loss: 4.604817  [44864/50000]
Test Error: 
 Accuracy: 3.3%, Avg loss: 4.603176 

Epoch 25
-------------------------------
loss: 4.601944  [   64/50000]
loss: 4.603979  [ 6464/50000]
loss: 4.603131  [12864/50000]
loss: 4.604357  [19264/50000]
loss: 4.602442  [25664/50000]
loss: 4.603308  [32064/50000]
loss: 4.603779  [38464/50000]
loss: 4.605301  [44864/50000]
Test Error: 
 Accuracy: 3.6%, Avg loss: 4.602917 

Epoch 26
-------------------------------
loss: 4.601478  [   64/50000]
loss: 4.603531  [ 6464/50000]
loss: 4.602339  [12864/50000]
loss: 4.601767  [19264/50000]
loss: 4.603248  [25664/50000]
loss: 4.603703  [32064/50000]
loss: 4.602699  [38464/50000]
loss: 4.602368  [44864/50000]
Test Error: 
 Accuracy: 4.0%, Avg loss: 4.602630 

Epoch 27
-------------------------------
loss: 4.603607  [   64/50000]
loss: 4.600793  [ 6464/50000]
loss: 4.602458  [12864/50000]
loss: 4.602696  [19264/50000]
loss: 4.603430  [25664/50000]
loss: 4.602293  [32064/50000]
loss: 4.602862  [38464/50000]
loss: 4.601201  [44864/50000]
Test Error: 
 Accuracy: 4.8%, Avg loss: 4.602267 

Epoch 28
-------------------------------
loss: 4.603053  [   64/50000]
loss: 4.602511  [ 6464/50000]
loss: 4.602084  [12864/50000]
loss: 4.603018  [19264/50000]
loss: 4.601194  [25664/50000]
loss: 4.601985  [32064/50000]
loss: 4.601184  [38464/50000]
loss: 4.601850  [44864/50000]
Test Error: 
 Accuracy: 5.2%, Avg loss: 4.601803 

Epoch 29
-------------------------------
loss: 4.603493  [   64/50000]
loss: 4.601743  [ 6464/50000]
loss: 4.601330  [12864/50000]
loss: 4.601347  [19264/50000]
loss: 4.602080  [25664/50000]
loss: 4.599683  [32064/50000]
loss: 4.601177  [38464/50000]
loss: 4.601048  [44864/50000]
Test Error: 
 Accuracy: 5.7%, Avg loss: 4.601237 

Epoch 30
-------------------------------
loss: 4.600821  [   64/50000]
loss: 4.600125  [ 6464/50000]
loss: 4.600387  [12864/50000]
loss: 4.599746  [19264/50000]
loss: 4.600889  [25664/50000]
loss: 4.601707  [32064/50000]
loss: 4.600083  [38464/50000]
loss: 4.601970  [44864/50000]
Test Error: 
 Accuracy: 6.1%, Avg loss: 4.600488 

Epoch 31
-------------------------------
loss: 4.600447  [   64/50000]
loss: 4.599902  [ 6464/50000]
loss: 4.599798  [12864/50000]
loss: 4.601304  [19264/50000]
loss: 4.600153  [25664/50000]
loss: 4.599768  [32064/50000]
loss: 4.601799  [38464/50000]
loss: 4.599053  [44864/50000]
Test Error: 
 Accuracy: 6.2%, Avg loss: 4.599497 

Epoch 32
-------------------------------
loss: 4.599550  [   64/50000]
loss: 4.597353  [ 6464/50000]
loss: 4.599227  [12864/50000]
loss: 4.600516  [19264/50000]
loss: 4.598032  [25664/50000]
loss: 4.598332  [32064/50000]
loss: 4.597224  [38464/50000]
loss: 4.599373  [44864/50000]
Test Error: 
 Accuracy: 5.7%, Avg loss: 4.598060 

Epoch 33
-------------------------------
loss: 4.598842  [   64/50000]
loss: 4.597895  [ 6464/50000]
loss: 4.596875  [12864/50000]
loss: 4.598371  [19264/50000]
loss: 4.598582  [25664/50000]
loss: 4.596906  [32064/50000]
loss: 4.596808  [38464/50000]
loss: 4.595859  [44864/50000]
Test Error: 
 Accuracy: 5.0%, Avg loss: 4.595870 

Epoch 34
-------------------------------
loss: 4.594301  [   64/50000]
loss: 4.595909  [ 6464/50000]
loss: 4.593670  [12864/50000]
loss: 4.593811  [19264/50000]
loss: 4.595886  [25664/50000]
loss: 4.592961  [32064/50000]
loss: 4.594007  [38464/50000]
loss: 4.590466  [44864/50000]
Test Error: 
 Accuracy: 4.3%, Avg loss: 4.592271 

Epoch 35
-------------------------------
loss: 4.597106  [   64/50000]
loss: 4.592058  [ 6464/50000]
loss: 4.592961  [12864/50000]
loss: 4.587052  [19264/50000]
loss: 4.586584  [25664/50000]
loss: 4.584609  [32064/50000]
loss: 4.587121  [38464/50000]
loss: 4.586904  [44864/50000]
Test Error: 
 Accuracy: 3.8%, Avg loss: 4.585862 

Epoch 36
-------------------------------
loss: 4.586791  [   64/50000]
loss: 4.586536  [ 6464/50000]
loss: 4.584864  [12864/50000]
loss: 4.583848  [19264/50000]
loss: 4.583167  [25664/50000]
loss: 4.576802  [32064/50000]
loss: 4.563583  [38464/50000]
loss: 4.578808  [44864/50000]
Test Error: 
 Accuracy: 3.0%, Avg loss: 4.573052 

Epoch 37
-------------------------------
loss: 4.582920  [   64/50000]
loss: 4.573042  [ 6464/50000]
loss: 4.570400  [12864/50000]
loss: 4.558462  [19264/50000]
loss: 4.556067  [25664/50000]
loss: 4.561844  [32064/50000]
loss: 4.559366  [38464/50000]
loss: 4.541192  [44864/50000]
Test Error: 
 Accuracy: 2.3%, Avg loss: 4.542470 

Epoch 38
-------------------------------
loss: 4.535735  [   64/50000]
loss: 4.544846  [ 6464/50000]
loss: 4.528705  [12864/50000]
loss: 4.535427  [19264/50000]
loss: 4.502330  [25664/50000]
loss: 4.502453  [32064/50000]
loss: 4.477966  [38464/50000]
loss: 4.462394  [44864/50000]
Test Error: 
 Accuracy: 2.2%, Avg loss: 4.453927 

Epoch 39
-------------------------------
loss: 4.396589  [   64/50000]
loss: 4.452935  [ 6464/50000]
loss: 4.406372  [12864/50000]
loss: 4.425340  [19264/50000]
loss: 4.222042  [25664/50000]
loss: 4.320349  [32064/50000]
loss: 4.311566  [38464/50000]
loss: 4.226825  [44864/50000]
Test Error: 
 Accuracy: 2.4%, Avg loss: 4.237179 

Epoch 40
-------------------------------
loss: 4.298555  [   64/50000]
loss: 4.270498  [ 6464/50000]
loss: 4.211253  [12864/50000]
loss: 4.123620  [19264/50000]
loss: 4.027475  [25664/50000]
loss: 4.067681  [32064/50000]
loss: 4.105279  [38464/50000]
loss: 4.087156  [44864/50000]
Test Error: 
 Accuracy: 4.0%, Avg loss: 4.069735 

Epoch 41
-------------------------------
loss: 4.044622  [   64/50000]
loss: 4.199889  [ 6464/50000]
loss: 4.099813  [12864/50000]
loss: 4.070475  [19264/50000]
loss: 4.016292  [25664/50000]
loss: 3.901017  [32064/50000]
loss: 3.913552  [38464/50000]
loss: 3.951098  [44864/50000]
Test Error: 
 Accuracy: 5.2%, Avg loss: 3.975941 

Epoch 42
-------------------------------
loss: 4.091998  [   64/50000]
loss: 4.026179  [ 6464/50000]
loss: 3.929505  [12864/50000]
loss: 3.881387  [19264/50000]
loss: 3.947963  [25664/50000]
loss: 3.813905  [32064/50000]
loss: 3.868939  [38464/50000]
loss: 3.906603  [44864/50000]
Test Error: 
 Accuracy: 5.5%, Avg loss: 3.909837 

Epoch 43
-------------------------------
loss: 3.883335  [   64/50000]
loss: 4.036110  [ 6464/50000]
loss: 3.851244  [12864/50000]
loss: 3.843326  [19264/50000]
loss: 4.052487  [25664/50000]
loss: 3.633350  [32064/50000]
loss: 3.721945  [38464/50000]
loss: 3.731245  [44864/50000]
Test Error: 
 Accuracy: 6.5%, Avg loss: 3.768651 

Epoch 44
-------------------------------
loss: 3.785637  [   64/50000]
loss: 3.676479  [ 6464/50000]
loss: 3.616360  [12864/50000]
loss: 3.610784  [19264/50000]
loss: 3.662137  [25664/50000]
loss: 3.522003  [32064/50000]
loss: 3.610968  [38464/50000]
loss: 3.465792  [44864/50000]
Test Error: 
 Accuracy: 8.1%, Avg loss: 3.560028 

Epoch 45
-------------------------------
loss: 3.598827  [   64/50000]
loss: 3.804907  [ 6464/50000]
loss: 3.381366  [12864/50000]
loss: 3.533159  [19264/50000]
loss: 3.329522  [25664/50000]
loss: 3.193981  [32064/50000]
loss: 3.444770  [38464/50000]
loss: 3.613120  [44864/50000]
Test Error: 
 Accuracy: 9.7%, Avg loss: 3.379305 

Epoch 46
-------------------------------
loss: 3.347117  [   64/50000]
loss: 3.379269  [ 6464/50000]
loss: 3.186142  [12864/50000]
loss: 3.046513  [19264/50000]
loss: 3.202506  [25664/50000]
loss: 3.499238  [32064/50000]
loss: 3.129446  [38464/50000]
loss: 3.146727  [44864/50000]
Test Error: 
 Accuracy: 9.4%, Avg loss: 3.315185 

Epoch 47
-------------------------------
loss: 3.016413  [   64/50000]
loss: 3.551098  [ 6464/50000]
loss: 3.536317  [12864/50000]
loss: 3.269629  [19264/50000]
loss: 3.352855  [25664/50000]
loss: 3.542553  [32064/50000]
loss: 3.024462  [38464/50000]
loss: 3.520077  [44864/50000]
Test Error: 
 Accuracy: 10.1%, Avg loss: 3.298983 

Epoch 48
-------------------------------
loss: 3.259678  [   64/50000]
loss: 3.369565  [ 6464/50000]
loss: 2.960328  [12864/50000]
loss: 3.234253  [19264/50000]
loss: 3.120835  [25664/50000]
loss: 3.090735  [32064/50000]
loss: 3.482967  [38464/50000]
loss: 3.194503  [44864/50000]
Test Error: 
 Accuracy: 11.8%, Avg loss: 3.207612 

Epoch 49
-------------------------------
loss: 3.108483  [   64/50000]
loss: 3.153049  [ 6464/50000]
loss: 3.230402  [12864/50000]
loss: 3.118241  [19264/50000]
loss: 3.127942  [25664/50000]
loss: 3.128993  [32064/50000]
loss: 3.008149  [38464/50000]
loss: 3.228476  [44864/50000]
Test Error: 
 Accuracy: 11.6%, Avg loss: 3.231421 

Epoch 50
-------------------------------
loss: 3.072568  [   64/50000]
loss: 3.233030  [ 6464/50000]
loss: 2.926842  [12864/50000]
loss: 3.021663  [19264/50000]
loss: 3.034710  [25664/50000]
loss: 3.101860  [32064/50000]
loss: 3.157491  [38464/50000]
loss: 3.108301  [44864/50000]
Test Error: 
 Accuracy: 11.9%, Avg loss: 3.259446 

Epoch 51
-------------------------------
loss: 3.116785  [   64/50000]
loss: 3.370786  [ 6464/50000]
loss: 3.179253  [12864/50000]
loss: 3.054248  [19264/50000]
loss: 3.283674  [25664/50000]
loss: 3.270549  [32064/50000]
loss: 3.327945  [38464/50000]
loss: 2.991980  [44864/50000]
Test Error: 
 Accuracy: 14.4%, Avg loss: 3.119027 

Epoch 52
-------------------------------
loss: 3.060941  [   64/50000]
loss: 3.105572  [ 6464/50000]
loss: 2.832181  [12864/50000]
loss: 3.153471  [19264/50000]
loss: 2.980220  [25664/50000]
loss: 3.121781  [32064/50000]
loss: 3.435319  [38464/50000]
loss: 3.002114  [44864/50000]
Test Error: 
 Accuracy: 12.8%, Avg loss: 3.187494 

Epoch 53
-------------------------------
loss: 3.140933  [   64/50000]
loss: 2.798500  [ 6464/50000]
loss: 2.932607  [12864/50000]
loss: 3.071424  [19264/50000]
loss: 3.073994  [25664/50000]
loss: 3.077385  [32064/50000]
loss: 3.144531  [38464/50000]
loss: 3.186068  [44864/50000]
Test Error: 
 Accuracy: 13.8%, Avg loss: 3.098883 

Epoch 54
-------------------------------
loss: 3.094832  [   64/50000]
loss: 2.819145  [ 6464/50000]
loss: 2.854512  [12864/50000]
loss: 2.967829  [19264/50000]
loss: 2.891737  [25664/50000]
loss: 2.863276  [32064/50000]
loss: 2.938892  [38464/50000]
loss: 2.974145  [44864/50000]
Test Error: 
 Accuracy: 16.4%, Avg loss: 2.963894 

Epoch 55
-------------------------------
loss: 2.949873  [   64/50000]
loss: 2.983897  [ 6464/50000]
loss: 2.910841  [12864/50000]
loss: 2.754233  [19264/50000]
loss: 2.987534  [25664/50000]
loss: 2.539967  [32064/50000]
loss: 2.690356  [38464/50000]
loss: 3.334081  [44864/50000]
Test Error: 
 Accuracy: 17.3%, Avg loss: 2.935062 

Epoch 56
-------------------------------
loss: 2.847611  [   64/50000]
loss: 2.752345  [ 6464/50000]
loss: 2.694222  [12864/50000]
loss: 2.774731  [19264/50000]
loss: 2.741228  [25664/50000]
loss: 2.792673  [32064/50000]
loss: 2.506852  [38464/50000]
loss: 3.099554  [44864/50000]
Test Error: 
 Accuracy: 16.2%, Avg loss: 3.029779 

Epoch 57
-------------------------------
loss: 3.174518  [   64/50000]
loss: 2.997141  [ 6464/50000]
loss: 2.655244  [12864/50000]
loss: 2.668025  [19264/50000]
loss: 2.893403  [25664/50000]
loss: 2.649933  [32064/50000]
loss: 2.886058  [38464/50000]
loss: 2.834186  [44864/50000]
Test Error: 
 Accuracy: 19.1%, Avg loss: 2.874536 

Epoch 58
-------------------------------
loss: 3.052565  [   64/50000]
loss: 3.006074  [ 6464/50000]
loss: 2.729091  [12864/50000]
loss: 2.591280  [19264/50000]
loss: 2.680549  [25664/50000]
loss: 3.010040  [32064/50000]
loss: 2.352172  [38464/50000]
loss: 2.579351  [44864/50000]
Test Error: 
 Accuracy: 20.5%, Avg loss: 2.796342 

Epoch 59
-------------------------------
loss: 2.755668  [   64/50000]
loss: 2.624870  [ 6464/50000]
loss: 2.548167  [12864/50000]
loss: 2.718041  [19264/50000]
loss: 2.941673  [25664/50000]
loss: 2.597982  [32064/50000]
loss: 2.537372  [38464/50000]
loss: 2.772489  [44864/50000]
Test Error: 
 Accuracy: 22.4%, Avg loss: 2.792161 

Epoch 60
-------------------------------
loss: 2.527047  [   64/50000]
loss: 2.511408  [ 6464/50000]
loss: 2.488653  [12864/50000]
loss: 2.504695  [19264/50000]
loss: 2.748082  [25664/50000]
loss: 2.394046  [32064/50000]
loss: 3.068879  [38464/50000]
loss: 2.950449  [44864/50000]
Test Error: 
 Accuracy: 23.1%, Avg loss: 2.708855 

Epoch 61
-------------------------------
loss: 2.610563  [   64/50000]
loss: 2.691975  [ 6464/50000]
loss: 2.830829  [12864/50000]
loss: 2.701911  [19264/50000]
loss: 2.615210  [25664/50000]
loss: 2.681405  [32064/50000]
loss: 2.683537  [38464/50000]
loss: 2.704811  [44864/50000]
Test Error: 
 Accuracy: 24.2%, Avg loss: 2.686416 

Epoch 62
-------------------------------
loss: 2.643957  [   64/50000]
loss: 2.956220  [ 6464/50000]
loss: 2.617216  [12864/50000]
loss: 2.627617  [19264/50000]
loss: 2.790261  [25664/50000]
loss: 2.742219  [32064/50000]
loss: 2.622174  [38464/50000]
loss: 2.975076  [44864/50000]
Test Error: 
 Accuracy: 25.6%, Avg loss: 2.627261 

Epoch 63
-------------------------------
loss: 2.644888  [   64/50000]
loss: 2.335080  [ 6464/50000]
loss: 2.818647  [12864/50000]
loss: 2.443903  [19264/50000]
loss: 2.642030  [25664/50000]
loss: 2.736547  [32064/50000]
loss: 2.681062  [38464/50000]
loss: 2.930354  [44864/50000]
Test Error: 
 Accuracy: 23.2%, Avg loss: 2.766801 

Epoch 64
-------------------------------
loss: 2.863750  [   64/50000]
loss: 2.730986  [ 6464/50000]
loss: 2.728623  [12864/50000]
loss: 2.502668  [19264/50000]
loss: 2.407943  [25664/50000]
loss: 2.408315  [32064/50000]
loss: 2.703650  [38464/50000]
loss: 2.706217  [44864/50000]
Test Error: 
 Accuracy: 25.8%, Avg loss: 2.645468 

Epoch 65
-------------------------------
loss: 2.578237  [   64/50000]
loss: 2.669262  [ 6464/50000]
loss: 2.303623  [12864/50000]
loss: 2.644929  [19264/50000]
loss: 2.395766  [25664/50000]
loss: 2.312269  [32064/50000]
loss: 2.537634  [38464/50000]
loss: 2.490801  [44864/50000]
Test Error: 
 Accuracy: 23.0%, Avg loss: 2.752410 

Epoch 66
-------------------------------
loss: 3.021421  [   64/50000]
loss: 2.346622  [ 6464/50000]
loss: 2.525347  [12864/50000]
loss: 2.541105  [19264/50000]
loss: 2.747716  [25664/50000]
loss: 2.528719  [32064/50000]
loss: 2.320998  [38464/50000]
loss: 2.480776  [44864/50000]
Test Error: 
 Accuracy: 27.4%, Avg loss: 2.564415 

Epoch 67
-------------------------------
loss: 2.492783  [   64/50000]
loss: 2.667328  [ 6464/50000]
loss: 2.583784  [12864/50000]
loss: 2.646022  [19264/50000]
loss: 2.756983  [25664/50000]
loss: 2.837039  [32064/50000]
loss: 2.582770  [38464/50000]
loss: 2.665853  [44864/50000]
Test Error: 
 Accuracy: 27.1%, Avg loss: 2.593189 

Epoch 68
-------------------------------
loss: 2.406660  [   64/50000]
loss: 2.624050  [ 6464/50000]
loss: 2.499320  [12864/50000]
loss: 2.449576  [19264/50000]
loss: 2.435842  [25664/50000]
loss: 2.846439  [32064/50000]
loss: 2.890319  [38464/50000]
loss: 2.364413  [44864/50000]
Test Error: 
 Accuracy: 27.3%, Avg loss: 2.548595 

Epoch 69
-------------------------------
loss: 2.292492  [   64/50000]
loss: 2.278699  [ 6464/50000]
loss: 2.668551  [12864/50000]
loss: 2.849924  [19264/50000]
loss: 2.681554  [25664/50000]
loss: 2.666552  [32064/50000]
loss: 2.279189  [38464/50000]
loss: 2.382657  [44864/50000]
Test Error: 
 Accuracy: 26.7%, Avg loss: 2.598485 

Epoch 70
-------------------------------
loss: 2.466434  [   64/50000]
loss: 2.421565  [ 6464/50000]
loss: 2.550320  [12864/50000]
loss: 2.565408  [19264/50000]
loss: 2.965059  [25664/50000]
loss: 2.759744  [32064/50000]
loss: 2.637815  [38464/50000]
loss: 2.078871  [44864/50000]
Test Error: 
 Accuracy: 27.5%, Avg loss: 2.574989 

Epoch 71
-------------------------------
loss: 2.593714  [   64/50000]
loss: 2.328136  [ 6464/50000]
loss: 2.678133  [12864/50000]
loss: 2.398591  [19264/50000]
loss: 2.270029  [25664/50000]
loss: 2.501182  [32064/50000]
loss: 2.441108  [38464/50000]
loss: 2.435777  [44864/50000]
Test Error: 
 Accuracy: 27.5%, Avg loss: 2.574506 

Epoch 72
-------------------------------
loss: 2.259571  [   64/50000]
loss: 2.618889  [ 6464/50000]
loss: 2.187310  [12864/50000]
loss: 2.427052  [19264/50000]
loss: 2.267575  [25664/50000]
loss: 2.530715  [32064/50000]
loss: 2.519275  [38464/50000]
loss: 2.611589  [44864/50000]
Test Error: 
 Accuracy: 29.5%, Avg loss: 2.482099 

Epoch 73
-------------------------------
loss: 2.253131  [   64/50000]
loss: 2.494345  [ 6464/50000]
loss: 2.383951  [12864/50000]
loss: 2.643687  [19264/50000]
loss: 2.614467  [25664/50000]
loss: 2.331319  [32064/50000]
loss: 2.337371  [38464/50000]
loss: 2.592049  [44864/50000]
Test Error: 
 Accuracy: 26.8%, Avg loss: 2.577385 

Epoch 74
-------------------------------
loss: 2.631534  [   64/50000]
loss: 2.600136  [ 6464/50000]
loss: 2.333732  [12864/50000]
loss: 2.370138  [19264/50000]
loss: 2.007613  [25664/50000]
loss: 2.263045  [32064/50000]
loss: 2.451803  [38464/50000]
loss: 2.393080  [44864/50000]
Test Error: 
 Accuracy: 31.8%, Avg loss: 2.403840 

Epoch 75
-------------------------------
loss: 2.368842  [   64/50000]
loss: 2.528388  [ 6464/50000]
loss: 2.334685  [12864/50000]
loss: 2.225677  [19264/50000]
loss: 2.206678  [25664/50000]
loss: 2.328578  [32064/50000]
loss: 2.343584  [38464/50000]
loss: 2.256380  [44864/50000]
Test Error: 
 Accuracy: 31.6%, Avg loss: 2.419409 

Epoch 76
-------------------------------
loss: 2.063737  [   64/50000]
loss: 2.372902  [ 6464/50000]
loss: 2.514768  [12864/50000]
loss: 2.357454  [19264/50000]
loss: 2.272247  [25664/50000]
loss: 2.332888  [32064/50000]
loss: 2.038347  [38464/50000]
loss: 2.305389  [44864/50000]
Test Error: 
 Accuracy: 31.4%, Avg loss: 2.400760 

Epoch 77
-------------------------------
loss: 2.575456  [   64/50000]
loss: 2.168599  [ 6464/50000]
loss: 2.359531  [12864/50000]
loss: 2.001158  [19264/50000]
loss: 2.288005  [25664/50000]
loss: 2.490213  [32064/50000]
loss: 2.359923  [38464/50000]
loss: 2.370541  [44864/50000]
Test Error: 
 Accuracy: 32.6%, Avg loss: 2.386055 

Epoch 78
-------------------------------
loss: 2.462538  [   64/50000]
loss: 2.247262  [ 6464/50000]
loss: 2.431804  [12864/50000]
loss: 2.121060  [19264/50000]
loss: 2.651342  [25664/50000]
loss: 2.204943  [32064/50000]
loss: 2.050377  [38464/50000]
loss: 2.481647  [44864/50000]
Test Error: 
 Accuracy: 32.0%, Avg loss: 2.390067 

Epoch 79
-------------------------------
loss: 2.544987  [   64/50000]
loss: 2.769084  [ 6464/50000]
loss: 2.000212  [12864/50000]
loss: 2.206973  [19264/50000]
loss: 2.011549  [25664/50000]
loss: 2.325222  [32064/50000]
loss: 2.442177  [38464/50000]
loss: 2.244292  [44864/50000]
Test Error: 
 Accuracy: 30.5%, Avg loss: 2.467398 

Epoch 80
-------------------------------
loss: 2.387990  [   64/50000]
loss: 2.214417  [ 6464/50000]
loss: 1.981644  [12864/50000]
loss: 2.509529  [19264/50000]
loss: 2.339295  [25664/50000]
loss: 2.440514  [32064/50000]
loss: 2.342620  [38464/50000]
loss: 2.155841  [44864/50000]
Test Error: 
 Accuracy: 34.4%, Avg loss: 2.323927 

Epoch 81
-------------------------------
loss: 2.502400  [   64/50000]
loss: 2.108599  [ 6464/50000]
loss: 1.786316  [12864/50000]
loss: 2.131862  [19264/50000]
loss: 2.057032  [25664/50000]
loss: 2.330945  [32064/50000]
loss: 2.131137  [38464/50000]
loss: 2.250016  [44864/50000]
Test Error: 
 Accuracy: 36.4%, Avg loss: 2.227451 

Epoch 82
-------------------------------
loss: 2.088417  [   64/50000]
loss: 2.417795  [ 6464/50000]
loss: 2.246053  [12864/50000]
loss: 2.308911  [19264/50000]
loss: 2.137691  [25664/50000]
loss: 2.128208  [32064/50000]
loss: 2.019979  [38464/50000]
loss: 2.234821  [44864/50000]
Test Error: 
 Accuracy: 37.2%, Avg loss: 2.210101 

Epoch 83
-------------------------------
loss: 2.126432  [   64/50000]
loss: 2.338345  [ 6464/50000]
loss: 2.259394  [12864/50000]
loss: 2.372143  [19264/50000]
loss: 1.914035  [25664/50000]
loss: 2.155087  [32064/50000]
loss: 2.208646  [38464/50000]
loss: 2.125573  [44864/50000]
Test Error: 
 Accuracy: 38.8%, Avg loss: 2.164858 

Epoch 84
-------------------------------
loss: 2.189510  [   64/50000]
loss: 1.890458  [ 6464/50000]
loss: 2.800578  [12864/50000]
loss: 2.001097  [19264/50000]
loss: 2.127834  [25664/50000]
loss: 2.060642  [32064/50000]
loss: 2.149614  [38464/50000]
loss: 2.485469  [44864/50000]
Test Error: 
 Accuracy: 37.2%, Avg loss: 2.214589 

Epoch 85
-------------------------------
loss: 2.782226  [   64/50000]
loss: 1.969863  [ 6464/50000]
loss: 2.248251  [12864/50000]
loss: 2.195993  [19264/50000]
loss: 2.126071  [25664/50000]
loss: 2.124428  [32064/50000]
loss: 2.011770  [38464/50000]
loss: 2.401991  [44864/50000]
Test Error: 
 Accuracy: 30.9%, Avg loss: 2.502530 

Epoch 86
-------------------------------
loss: 2.085921  [   64/50000]
loss: 2.079611  [ 6464/50000]
loss: 2.124931  [12864/50000]
loss: 2.026773  [19264/50000]
loss: 2.288070  [25664/50000]
loss: 2.314934  [32064/50000]
loss: 1.871850  [38464/50000]
loss: 1.766823  [44864/50000]
Test Error: 
 Accuracy: 38.1%, Avg loss: 2.179698 

Epoch 87
-------------------------------
loss: 2.009198  [   64/50000]
loss: 2.176783  [ 6464/50000]
loss: 1.912450  [12864/50000]
loss: 2.116534  [19264/50000]
loss: 2.038009  [25664/50000]
loss: 2.071680  [32064/50000]
loss: 1.974033  [38464/50000]
loss: 2.097601  [44864/50000]
Test Error: 
 Accuracy: 38.7%, Avg loss: 2.140806 

Epoch 88
-------------------------------
loss: 2.047157  [   64/50000]
loss: 1.755619  [ 6464/50000]
loss: 1.910724  [12864/50000]
loss: 2.151356  [19264/50000]
loss: 2.448027  [25664/50000]
loss: 2.042119  [32064/50000]
loss: 1.936654  [38464/50000]
loss: 2.084266  [44864/50000]
Test Error: 
 Accuracy: 38.8%, Avg loss: 2.175640 

Epoch 89
-------------------------------
loss: 1.844752  [   64/50000]
loss: 1.657503  [ 6464/50000]
loss: 2.162173  [12864/50000]
loss: 1.846154  [19264/50000]
loss: 2.152461  [25664/50000]
loss: 2.035107  [32064/50000]
loss: 1.918579  [38464/50000]
loss: 2.149945  [44864/50000]
Test Error: 
 Accuracy: 36.6%, Avg loss: 2.235654 

Epoch 90
-------------------------------
loss: 2.026552  [   64/50000]
loss: 2.097789  [ 6464/50000]
loss: 1.770280  [12864/50000]
loss: 1.748509  [19264/50000]
loss: 1.825913  [25664/50000]
loss: 2.071578  [32064/50000]
loss: 1.791667  [38464/50000]
loss: 1.733401  [44864/50000]
Test Error: 
 Accuracy: 39.5%, Avg loss: 2.115550 

Epoch 91
-------------------------------
loss: 2.033822  [   64/50000]
loss: 2.162895  [ 6464/50000]
loss: 2.091838  [12864/50000]
loss: 1.630894  [19264/50000]
loss: 2.034765  [25664/50000]
loss: 2.193156  [32064/50000]
loss: 2.095063  [38464/50000]
loss: 1.803363  [44864/50000]
Test Error: 
 Accuracy: 37.7%, Avg loss: 2.237137 

Epoch 92
-------------------------------
loss: 1.774130  [   64/50000]
loss: 1.667038  [ 6464/50000]
loss: 2.048494  [12864/50000]
loss: 2.240629  [19264/50000]
loss: 1.977379  [25664/50000]
loss: 1.856467  [32064/50000]
loss: 1.864844  [38464/50000]
loss: 1.902884  [44864/50000]
Test Error: 
 Accuracy: 41.9%, Avg loss: 2.017277 

Epoch 93
-------------------------------
loss: 2.089393  [   64/50000]
loss: 1.912923  [ 6464/50000]
loss: 2.174060  [12864/50000]
loss: 2.264884  [19264/50000]
loss: 2.028558  [25664/50000]
loss: 1.900539  [32064/50000]
loss: 2.065797  [38464/50000]
loss: 1.916407  [44864/50000]
Test Error: 
 Accuracy: 41.8%, Avg loss: 2.025498 

Epoch 94
-------------------------------
loss: 2.074182  [   64/50000]
loss: 1.564890  [ 6464/50000]
loss: 1.827641  [12864/50000]
loss: 2.040174  [19264/50000]
loss: 1.671355  [25664/50000]
loss: 2.358469  [32064/50000]
loss: 1.909292  [38464/50000]
loss: 2.141831  [44864/50000]
Test Error: 
 Accuracy: 42.5%, Avg loss: 2.024341 

Epoch 95
-------------------------------
loss: 1.751814  [   64/50000]
loss: 1.907760  [ 6464/50000]
loss: 1.775368  [12864/50000]
loss: 1.788905  [19264/50000]
loss: 2.219922  [25664/50000]
loss: 2.216849  [32064/50000]
loss: 2.554458  [38464/50000]
loss: 1.800615  [44864/50000]
Test Error: 
 Accuracy: 39.4%, Avg loss: 2.147581 

Epoch 96
-------------------------------
loss: 2.041896  [   64/50000]
loss: 2.039407  [ 6464/50000]
loss: 2.207699  [12864/50000]
loss: 1.614774  [19264/50000]
loss: 2.009202  [25664/50000]
loss: 1.985621  [32064/50000]
loss: 1.379539  [38464/50000]
loss: 2.362211  [44864/50000]
Test Error: 
 Accuracy: 43.2%, Avg loss: 2.016317 

Epoch 97
-------------------------------
loss: 1.604933  [   64/50000]
loss: 2.133072  [ 6464/50000]
loss: 1.864662  [12864/50000]
loss: 1.616036  [19264/50000]
loss: 1.905701  [25664/50000]
loss: 2.108338  [32064/50000]
loss: 1.891636  [38464/50000]
loss: 1.841889  [44864/50000]
Test Error: 
 Accuracy: 39.5%, Avg loss: 2.124020 

Epoch 98
-------------------------------
loss: 1.857156  [   64/50000]
loss: 1.856735  [ 6464/50000]
loss: 2.124340  [12864/50000]
loss: 1.619845  [19264/50000]
loss: 1.929067  [25664/50000]
loss: 1.709637  [32064/50000]
loss: 1.734665  [38464/50000]
loss: 1.721989  [44864/50000]
Test Error: 
 Accuracy: 43.5%, Avg loss: 2.000441 

Epoch 99
-------------------------------
loss: 2.018298  [   64/50000]
loss: 1.781956  [ 6464/50000]
loss: 1.845252  [12864/50000]
loss: 1.954304  [19264/50000]
loss: 2.201977  [25664/50000]
loss: 2.243520  [32064/50000]
loss: 1.466485  [38464/50000]
loss: 1.794748  [44864/50000]
Test Error: 
 Accuracy: 41.7%, Avg loss: 2.052778 

Epoch 100
-------------------------------
loss: 2.061980  [   64/50000]
loss: 1.526264  [ 6464/50000]
loss: 1.998305  [12864/50000]
loss: 1.811208  [19264/50000]
loss: 1.978089  [25664/50000]
loss: 1.786697  [32064/50000]
loss: 1.945079  [38464/50000]
loss: 1.785952  [44864/50000]
Test Error: 
 Accuracy: 45.0%, Avg loss: 1.943056 

Done!
Loaded results from results/CIFAR100.csv
Model already exists in results, will remove old results
Saved results to results/CIFAR100.csv
Saved PyTorch Model State to saved_models/RN50_clip_FC_FF_NN.pth
Loaded results from results/CIFAR100.csv
End Time: Wed 22 Mar 17:35:33 GMT 2023
