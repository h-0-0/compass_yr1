\section{Introduction}
In this report we investigate the effect of latent representations on continual learning performance. Continual learning \cite{de2021continual, parisi2019continual} describes a challenge were we try and train models on a series of tasks that are presented to the network in a sequential fashion, we are specifically concerned with the subset of continual learning where neural network architectures are the models being trained. We will also be mainly concerned with the problem of continually learning an image classification task. Usually these tasks contain very different data such as distinct classes and this means the assumption that our data is independently identically distributed, which we need to perform minibatch stochastic gradient descent, is broken. This leads to the problem of catastrophic forgetting \cite{ratcliff1990catastrophic, mccloskey1989catastrophic, french1999catastrophic} where a neural network will forget to correctly classify data from a previous task whilst learning on a new task, which means after sequentially learning a series of tasks the network will only be able to correctly classify images (with good performance) that are similar to data presented in the last task. 

We aimed through empirical analysis to investigate how using fixed latent representations might affect the degree to which a network forgets and also how we learnt the latent representation and what we learned it from might affect the degree of forgetting. We first demonstrated the problem of forgetting using simple networks, a fully connected network and a small convolutional neural network \cite{lecun1995convolutional}, on a simple continual learning dataset the MNIST handwritten digits dataset \cite{deng2012mnist} partitioned into 5 tasks where each task contains all the data corresponding to two classes (two digits). We found that the networks immediately forgot all knowledge on the previous task as soon as they started training on a new task. 

We then carried out experiments using Resnet18, VGG16 and autoencoder networks as encoders with some fully connected layers on the end for classification. These networks are all of varying sizes and had either no pretraining or were trained on CIFAR100 or Imagenet. For each network we also had two variations: one where the encoder was frozen and one where the encoder was trainable. We found that by the end of training on any task the previous task had been completely forgotten, ie. achieves an accuracy of zero or near zero on the previous task. Furthermore, we found that bar the network with frozen Resnet18 as the encoder every network immediately forgot all knowledge on the previous task (ie. achieving a training and testing accuracy of zero or very near zero) after just one epoch of training on the next task. This severe forgetting is somewhat surprising given the lesser extent of forgetting that has been shown in other papers such as in \cite{ramasesh2022effect,toneva2018empirical}, however, our increased number of tasks, smaller networks and not conducting interleaved training could explain our more severe results. Our results more closely resemble the kind of forgetting shown in \cite{mccloskey1989catastrophic}, however, the networks and datasets used to obtain the results are extremely simple. 

Our results from the network with frozen Resnet18 as the encoder showed some more gradual forgetting on some tasks. In \cite{ramasesh2022effect} the authors found that larger networks and larger pre-training datasets led to reduced forgetting which our results compliment as Resnet18 is the largest network that we experimented with, used the largest pretraining dataset we used (Imagenet) and was the only network to show more gradual forgetting. However, this doesn't explain why the non-frozen version of this network didn't also demonstrate slightly more gradual forgetting. This could suggest further avenues of exploration including conducting experiments on split-CIFAR10 with 5 tasks using larger networks with larger pre-training datasets and like we have done in this report comparing the amount of forgetting that occurs when we use a frozen encoder versus a non-frozen encoder. Our result with the frozen Resnet18 hints at the possibility that frozen representations might somewhat mitigate forgetting, which would run in opposition to what was found in \cite{ramasesh2020anatomy} where they found that almost all forgetting happens in the deepest layers but may agree with the work in \cite{rebuffi2017icarl} and \cite{li2017learning} where it appears that when training on tasks sequentially the encoder was mainly changed and not the classifier layers. An initial investigation into using larger networks showed even more gradual forgetting when the encoder was frozen (but not when the encoder was not trainable), suggesting there may be benefit to using fixed latent representations to hamper forgetting, further more rigorous investigation is of course required. However, we observed no changing in forgetting in any of the other networks when freezing the encoder which somewhat gives support to the findings in \cite{ramasesh2020anatomy}, that all or most forgetting happens in the deepest layers.

We also experimented with randomly initializing the fully connected, classifier, portions of the network at the end of each task and demonstrated that our results for the large part resembled the results without resetting the classifier. This is due to the fact that complete forgetting (accuracy on previously trained tasks is zero or near zero) is so immediate and that with a randomly initialized classifier the network is very quickly able to learn the new task. Motivated by these results we also introduced a novel CL technique were we solve the problem of CF by simply focusing on the deepest layers of the network. By simply training a new classifier head on each task we can achieve just as good accuracy on each task by the end of training and have zero forgetting as we don't train the classifier on any of the other tasks. This is a very simple and effective technique to implement, however, it requires us to store a separate classifier for each task and we must have knowledge as to what task we are currently training on.

All the code needed to produce the results in this report can be found on github at \url{https://github.com/h-aze/compass_yr1/tree/master/Mini_Project}. In the spirit of reproducibility all the results were obtained using a seeded random number generator (the seed used is the default seed). And all the experiments were run on one NVIDIA GeForce RTX 2080 Ti GPU. 