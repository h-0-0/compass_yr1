\section{Background}
In this section we will outline what the problem of catastrophic forgetting is, and how it is related to the area of continual learning. We will also discuss the three main approaches to continual learning, and the different types of encoders and deep learning architectures that are used in this report to demonstrate and evaluate how forgetting occurs in neural networks. 

\subsection{Deep Learning}
\label{subsec:DL}
Deep learning is a subfield of machine learning that uses a combination of a neural network and backpropagation to learn a model from data \cite{lecun2015deep,rumelhart1986learning,Goodfellow-et-al-2016,schmidhuber2015deep}. The  ``multi layer perceptron'' or ``feed forward neural network'' is the classic version of a neural network in which there are layers made up of neurons (or perceptrons), each neuron in a given layer is connected to every neuron in the next layer (unless it's a neuron in the output layer) and to every neuron in the previous layer (unless it's the input layer). A network with more than one hidden layer is called a ``deep neural network'', hence the name Deep Learning (DL) and the model is inspired from the structure of the brain. A neuron in a hidden layer (a layer that is neither the input nor output layer) or output layer acts as a function, it computes a weighted sum of the outputs of all the neurons in the previous layer and adds a term called the bias, it then applies what is called an activation function to the weighted sum. An activation function is a differentiable non-linear function such as the Rectified Linear Unit (ReLU) function \cite{nair2010rectified} which is the activation function we used in all of our neural architectures in our experiments. The ReLU function can be defined as follows:
\begin{equation}
    \sigma_{ReLU}(x) = \max(0,x)
\end{equation} 
where $\max$ simply returns the larger of its two arguments. The neurons in the input layer each represent a feature of the input data. A neural network can then be written as a function $f$ that takes in a vector of features $x$ and outputs a vector of predictions $y$, ie. y = f(x), where if $f$ is a network with one hidden layer then:
\begin{equation}
    f(x) := \sigma(\mathbf{W}_2 \sigma(\mathbf{W}_1 x + \mathbf{b}_1) + \mathbf{b}_2)
\end{equation}
where $\sigma$ is the activation function, $\mathbf{W}_1$ and $\mathbf{W}_2$ are the weight matrices, and $\mathbf{b}_1$ and $\mathbf{b}_2$ are the bias vectors. The weights and biases are learned by backpropagation, which is a method of training a neural network that uses gradient descent to minimise the loss function. The loss function is a function that measures how well the network is performing, it is usually the mean squared error (if doing regression for example) or cross entropy (if doing classification). The loss function is then differentiated with respect to the weights and biases, and the weights and biases are updated in a direction that minimizes the loss function. This process is repeated until the loss function is minimized. The way backpropagation works is by propagating the error backwards through the network, we calculate the errors like so:
\begin{align}
    \delta_2 = (a_2 - y) * g'(z_2) \\
    \delta_1 = (W_2)^T * \delta_2 * g'(z_1) \\
\end{align}
where $a_2$ is the output of the network, $y$ is the target output, $g$ is the activation function, $z_2$ is the weighted sum of the output layer, $z_1$ is the weighted sum of the hidden layer, and $\delta_2$ and $\delta_1$ are the error terms for the output and hidden layers respectively. The error terms are then used to update the weights and biases as follows:
\begin{align}
    W_2 \leftarrow W_2 - \alpha * \delta_2 * (a_1)^T \\
    b_2 \leftarrow b_2 - \alpha * \delta_2 \\
    W_1 \leftarrow W_1 - \alpha * \delta_1 * x^T \\
    b_1 \leftarrow b_1 - \alpha * \delta_1 
\end{align}
where $\alpha$ adjusts the rate at which we adapt our parameters (the learning rate). The key thing to note here is that the error terms which we use to calculate our parameter updates are calculated using the loss our network obtains on the current input\footnote{Now, in reality we don't update the network after every input, we calculate the error and gradients for each input in a batch of inputs and then average the gradients and update the network once (called mini-batch gradient descent). This saves computation whilst also making our updates to the network more stable}. Allegorically, we propagate the error backwards through the network to find which weights and biases are responsible for the error, and then we update those weights and biases accordingly to reduce the error. We will discuss why this is important in the context of catastrophic forgetting in \ref{subsec:CF}. 

This network resembles some of the first network architectures to appear in the literature, since then more complex neural architectures have arisen that illustrate better performance in many different scenarios. One such network is the Convolutional Neural Network (CNN) \cite{lecun1995convolutional}. CNN's take inspiration from the visual cortex which has been demonstrated to have a hierarchical architecture \cite{hubel1977ferrier}. A particular area it has shown impressive performance is in image classification which is the problem area we will focus on in this report. To put it simply it works by looking at a group of pixels, extracting information using a filter (a matrix of weights) and then applying a non-linear activation function to the result. The filter is then moved across the image (convolved), the process is then repeated on the output of this process. By repeating this process on the output of the previous layer we can extract more and more complex features from the image, typically we then feed the output of the convolutional layers to a small fully connected network to classify the image. We train the networks parameters using backpropagation in the exact same way as with a fully connected network.

\subsection{Encoders}
\label{subsec:encoders}
An encoder is a function we can use to give a useful, usually lower-dimensional, representation of data. In the context of machine learning an encoder entails algorithms such as Principal Component Analysis (PCA) \cite{hotelling1933analysis} and t-SNE \cite{van2008visualizing} which are techniques used to extract useful features from data. These features can then be used directly for analysis or as input to another model. What we will focus on are techniques that make use of neural architectures to encode data into useful latent representations. 

One such technique involves training networks on an image classification task and then extracting and using a portion of the network as an encoder. CNN's for example are often used as encoders, the network is trained on an image classification task, we then take the convolutional part of the network (ie. get rid of the fully connected portion) and make this our encoder. In order to classify the image the network has to extract useful features from the image therefore we can use the portion of the network where it is extracting these features as an encoder. An example of a specific CNN architecture that is widely used due to its good performance is the VGG16 \cite{simonyan2014very} network. The network has 16 layers, 13 convolutional layers and 3 fully connected layers. We used the convolutional portion of this network as one of our encoders in our experiments. 

Other techniques involve more thought into how to train a network to produce robust and informative latent representations, two of the most famous techniques that achieve this are Auto Encoders (AEs) \cite{hinton1993autoencoders,schmidhuber2015deep,Goodfellow-et-al-2016} and Variational Auto Encoders (VAEs) \cite{kingma2013auto}. AEs are a type of neural network that are trained to reconstruct their input. They are constructed in a way that the dimensionality of the layers initially decreases and then increases again where the output is the same dimensionality as the input. The network is optimized to minimize the reconstruction loss which measures the difference between the input to the network and the output of the network, this trains the network to create a good representation of the input that can be modelled by the bottleneck layer\footnote{The bottleneck layer is the layer that is the smallest in dimensionality, it is the layer that the network uses to represent the input.}. The better the encoding at the bottleneck layer the better it will be at being able to accurately reconstruct the input when decoding. 

The VAE differs from the AE in that it models the latent space as a probability distribution, specifically the Gaussian distribution, this allows us to sample from the latent space and generate new data (ie. it is a generative model). This has the added affect of making the latent space more robust and informative. It achieves this by having two heads on the bottleneck layer, one that outputs the mean of the latent space and one that outputs the variance of the latent space. The network is then optimized to minimize the reconstruction loss and the KL divergence between the latent space and a standard Gaussian distribution.

\subsection{Catastrophic Forgetting}
\label{subsec:CF}
The problem of catastrophic forgetting first appeared at the end of the 1980s, where it was observed that multilayer perceptron models trained (using backpropagation) on tasks sequentially incurred decreased performance on past tasks that they had been trained on \cite{ratcliff1990catastrophic,mccloskey1989catastrophic,french1999catastrophic}, this problem was named Catastrophic Forgetting (CF). Since, it has been observed in a whole variety of network architectures trained using backpropagation outside the classic multilayer perceptron such as in CNN's \cite{arora2019does} and in LSTM's \cite{schak2019study}. Precisely we can describe CF as follows:
\begin{definition}
    \textbf{Catastrophic Forgetting (CF):} The phenomenon where a neural network trained on a series of tasks sequentially incurs decreased performance on past tasks that it has been trained on. Catastrophic forgetting in the area of image classification will present itself as a decrease in the accuracy obtained on past tasks. We will use the term ``complete forgetting'' to refer to when the network achieves zero or close to zero percent accuracy.
\end{definition}
The problem of CF occurs due to the way we train these neural architectures, ie. due to backpropagation. As we mentioned in \ref{subsec:DL} a neural network is trained by propagating loss backwards through the network, finding which weights were responsible for the loss incurred and updating them accordingly so if we were to calculate the loss on the same data again it would be less. If we are working within the assumption that the samples of the data used to construct the mini-batches are all independently and identically distributed (iid.) then training this way causes no problems as on average we will be getting a good estimate of the loss on the data as a whole. However, if we are training on data that is not iid. then we will consistently get an unrepresentative value of the loss on the data as a whole and in turn change the network in such a way that it will not perform equally well on all the data. This is the case if we train on tasks sequentially, in this case when optimizing the network on the current task weights throughout the network will be changed to improve the performance on the current data being trained on, possibly at the detriment of the performance on the previous task, incurring forgetting.

Research in the area of understanding CF is sparse, especially in comparison to the amount of research that aims to mitigate it, and so deep understanding on the problem is somewhat limited. 

In \cite{ramasesh2020anatomy} the authors conducted a very comprehensive empirical study on where in the architecture forgetting most occurs and also analyzed how task semantics change the degree of forgetting. They investigated where in the network forgetting occurs by carrying out experiments on split-CIFAR10 that included freezing layers, resetting layers and analyzing hidden representation (network weights) similarity using representational similarity measures \cite{raghu2017svcca,kornblith2019similarity}. Their conclusion was that forgetting occurs in deeper layers of the network and interestingly they were also able to show that CL techniques such as EWC \cite{kirkpatrick2017reg} and experience replay \cite{rolnick2019replay} seemed to reduce forgetting by stabilizing deeper layers. They also showed that the degree of forgetting was dependent on the task semantics and through combination of empirical investigation and analytical model concluded that forgetting is most prevalent when the tasks have intermediate semantic similarity and that forgetting is reduced when the tasks are semantically either very similar or very different.

The findings above are confirmed by \cite{thai2021does}, where using their novel forgetting measure (DyRT) they were able to show that the bulk of forgetting occurs in the deepest layers (the classification layers) as opposed to the feature extraction portion of the network. This and the above is in contrast, however, to the work in \cite{rebuffi2017icarl} and \cite{li2017learning} where it appears that when training on tasks sequentially the encoder was mainly changed and not the classifier layers. 

Additionally, in \cite{thai2021does}, they investigated if catastrophic forgetting occurs when performing continual reconstruction, ie. reconstruction tasks\footnote{Reconstruction tasks involve optimizing for encoding data and then from the encoding reconstructing the original input, ie. what the AE and VAE do that we mentioned in \ref{subsec:encoders}.} in the continual learning setting. Empirical testing on 3D shape reconstruction datasets found that no CF was incurred when performing continual reconstruction versus in the batch scenario and in fact found better performance in some cases. They also confirmed this result in the 2D continual image reconstruction scenario on CIFAR100 and found similar results. These results are quite surprising as to our knowledge they are the only example of where CF in neural architectures has been shown to not occur without any sort of CL technique in place.

In \cite{ramasesh2022effect} the authors investigate the effect of using pretrained models on catastrophic forgetting. In particular they investigate how model size and size of the pretraining dataset affects the amount of forgetting. They find that pretrained models forget less than models trained from scratch and that the bigger the model and pretraining dataset the bigger this effect is. 

In \cite{goodfellow2013empirical} the authors investigate the effect of the activation function used on CF. There also exists literature which introduce methods aimed at visualizing CF, such as \cite{gigante2019visualizing,nguyen2020dissecting}.

\subsection{Continual Learning}
\label{subsec:CL}
Continual learning (CL) \cite{parisi2019continual} is a field in DL that focuses on developing techniques to allow networks to get good performance on problems that involve learning sequentially on tasks. This is a very different scenario to what DL models are normally trained to work in where you must have access to all the data you want the model to learn from at training time, known as the batch scenario. We now provide a formal definition of CL:
\begin{definition}
    \textbf{Continual Learning (CL):} A model that has the ``ability to continually learn over time by accommodating new knowledge while retaining previously learned experiences''\cite{parisi2019continual} is described as a continually learning model. 
\end{definition}
One of the main problems encountered when learning sequentially is CF, however, there are also problems such as catastrophic remembering \cite{sharkey1995analysis}. Continual learning is often described as the problem of solving the stability-plasticity dilema which means balancing the ability of the network to learn from the current task (its ability to be plastic) with its ability to not change too much as to not incur decreased performance on the previous task (its ability to be stable). The term task here is quite loose and can be defined in many ways, for example, it could be a different dataset, a different distribution on the same dataset, a different class in the same dataset, etc. For our purposes we will be looking at the case where each task contains a selection of classes from the same image classification dataset, ie. we partition the dataset into groups according to classes and label each of these groups as a different task, which is known as the problem of task incremental learning and is seen as one of the easier CL set-ups. Harder CL scenarios include class incremental learning where we learn sequentially each class and CL scenarios with no task or class boundaries (ie. we don't have knowledge as to what task or class we are training on). Something to note is we won't restrict ourselves to the data-stream task incremental learning set-up where we are only allowed to see each sample once, which is a harder subset of the task and class incremental scenarios. 

The area of CL saw some initial work in the very early days when the problem of CF was identified \cite{ratcliff1990catastrophic,french1999catastrophic} and has since seen a lot of work in the last few years \cite{de2021continual,parisi2019continual}. Most of the work in this area comes under three main approaches to solving the issue: (1) Regularization techniques \cite{kirkpatrick2017reg,zenke2017reg} which aim to reduce forgetting by adding penalties for changing parameters in the network (2) Replay techniques \cite{shin2017replay,rolnick2019replay} which aim to reduce forgetting by either replaying old data to the network or learning to generate synthetic data and replay that to the network (3) Architectural approaches \cite{mallya2018paramiso,rusu2016progressive} which aim to solve the problem by dynamically growing, pruning and freezing the network to mitigate forgetting. As a special mention, within the area of replay methods there exist methods that look at replaying latent representations of activations in the network to itself. This is known as latent replay, these methods in particular are somewhat relevant to our work as they make extensive use of foundational models. In \cite{ostapenko2022continual} the authors carry out an extensive set of experiments on latent replay methods, including experiments pitting frozen encoders against models trained end to end. They found that using frozen encoders greatly reduced compute and that in some cases all they needed was a non-parametric model in conjunction with the encoder to mitigate forgetting by a substantial amount.