\section{Conclusion}
In this report we introduced and discussed the problem of CF and how it manifests itself when trying to carry out continual learning with neural networks trained on image classification tasks. We then demonstrated the problem of catastrophic forgetting using simple networks, a fully connected network and a small convolutional neural network \cite{lecun1995convolutional}, on a simple continual learning dataset the MNIST handwritten digits dataset \cite{deng2012mnist} partitioned into 5 tasks where each task contains all the data corresponding to two classes (two digits). Where we found the networks to immediately get zero or near zero accuracy on testing and training data from the previous task as soon as they started training on a new task. 

We then carried out experiments on the split-CIFAR10 dataset with 5 tasks where each task contained all the data belonging to two classes. We again found that all networks immediately forgot all knowledge on the previous task after just one epoch of training on the next task, except for the fResnet18-FCFFNN network which exhibited some more gradual forgetting. We found this very severe forgetting somewhat surprising given less severe forgetting has been shown to occur in the literature such as in \cite{ramasesh2022effect,toneva2018empirical}, however, our increased number of tasks, smaller networks and not conducting interleaved training could explain our more severe results. We also remarked that our forgetting more closely resembles the severity of forgetting shown in \cite{mccloskey1989catastrophic} which used extremely simple and small networks on very small and simple datasets. We hypothesized that the reason fResnet18-FCFFNN was able to show more gradual forgetting could be down to its larger size and pretraining dataset which would agree with the findings in \cite{ramasesh2022effect}, however, couldn't solace as to why the non-frozen version wasn't also able to show more gradual forgetting. On one hand it seems our results match up with what was found in \cite{ramasesh2020anatomy} where the authors demonstrated that forgetting occurs almost exclusively in the deepest layers. However, on the other hand the ability of fResnet18-FCFFNN but not nfResnet18-FCFFNN to show more gradual forgetting could suggest that freezing shallow encoder layers might somewhat help to mitigate forgetting which may oppose the findings in \cite{ramasesh2020anatomy}. Our very brief initial investigation into using larger networks for the encoder (Resnet50) provide some initial evidence that would support the latter statement, however, further more rigorous investigation is required.

We also ran some experiments where we reset the parameters of the fully connected layers, of the networks which made use of pretrained encoders, to random values at the end of each task. We showed that the results using this resetting technique were extremely similar (bar results with fResnet18-FCFFNN) to the results without resetting. Using these results we proposed a CL technique which could solve the CF problem in a naive way by having a separate classifier for each task. This could achieve similar accuracies to training a network sequentially whilst sidestepping the forgetting problem by only training each classifier on their assigned task. Although we acknowledged the drawbacks of such a technique, mainly that we must have a classifier for each task and we must have knowledge on what task we are currently training on. Further investigation of course is required to determine whether or not this technique is viable in practice on other datasets and using different networks. 

In summary, we propose that further investigation is required in order to draw a conclusion as to whether or not using frozen latent representations might reduce the rate of forgetting in neural networks. We propose that experiments are carried out using much larger networks than we have here which have been pretrained on large rich datasets such as Imagenet. In these investigations we also propose that a mixture of encoders taken from image classification models and encoders taken from models trained on image reconstruction are used, to further explore the effect of the type of latent representation that has been trained on the degree of forgetting. In particular, we propose that the VAE \cite{kingma2013auto} is experimented with due to the more robust latent space it's capable of learning and that these networks trained to optimize reconstruction loss are trained on much larger datasets (even larger than Imagenet) which are easier to find due to the fact that they don't require labels. Another avenue of investigation could be empirically testing the amount of forgetting that occurs based on the task number selected for split datasets such as split-MNIST, split-CIFAR10 and split-CIFAR100. 
