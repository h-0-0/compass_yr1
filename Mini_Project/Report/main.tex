% !TEX root = /home/jd18380/Documents/compass_yr1/Mini_Project/Report/main.tex
\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\usepackage{booktabs}
\usepackage{tabu}
\usepackage[T1]{fontenc}
% \usepackage{subfig}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{multirow}

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=3cm,left=2cm,right=2cm,marginparwidth=2.75cm]{geometry}
%% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{xcolor}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
%\usepackage{apacite}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{question}{Question}[section]

\title{Investigating the Effect of Latent Representations on Continual Learning Performance}
\author{By Henry Bourne, Supervised by Rihuan Ke}
\date{}

\begin{document}
\maketitle


\begin{abstract}
    In this report we demonstrate how neural networks trained on tasks sequentially forget how to perform previous tasks they've trained on when training on new tasks. We demonstrate forgetting on the split-MNIST using simple networks and then on split-CIFAR10 with networks composed of a pretrained encoder and a fully connected classifier on the end where the encoders were trained using different techniques. We found that using frozen latent representations or trainable latent representations had no effect on the forgetting rate of the network, however, trainable latent representations tended to be able to achieve a higher accuracy on the task currently being trained. We also found all our networks except the largest, trained on the largest pretraining dataset with a frozen latent representation had immediate complete forgetting of the previous task after one epoch of training on a new task. We also propose an extremely simple continual learning technique based on our findings.  
\end{abstract}

\input{1_Intro}
\FloatBarrier
\input{2_Background}
\FloatBarrier
\input{3_Related}
\FloatBarrier
\input{4_Method}
\FloatBarrier
\input{5_Experiment}
\FloatBarrier
\input{6_Conclusion.tex}
\FloatBarrier
\small
\bibliographystyle{plain}
\bibliography{refs}
\input{appendix}
\end{document}